# ç¡¬ä»¶è‡ªé€‚åº”ä¼˜åŒ–ç³»ç»Ÿ - æŠ€æœ¯è®¾è®¡æ–‡æ¡£

**ç‰ˆæœ¬**: 1.0.0
**åˆ›å»ºæ—¥æœŸ**: 2025-10-14
**æ–‡æ¡£çŠ¶æ€**: è®¾è®¡ææ¡ˆ (Proposal)
**ç›®æ ‡ç‰ˆæœ¬**: NB10 v2.0.0

---

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

æœ¬æ–‡æ¡£æè¿°äº†ä¸€ä¸ª**æ™ºèƒ½ç¡¬ä»¶æ„ŸçŸ¥ä¸è‡ªé€‚åº”é…ç½®ç³»ç»Ÿ**çš„å®Œæ•´è®¾è®¡æ–¹æ¡ˆï¼Œæ—¨åœ¨è®©NB10 AI-CACå·¥å…·èƒ½å¤Ÿï¼š

1. **è‡ªåŠ¨æ£€æµ‹**ç”¨æˆ·çš„ç¡¬ä»¶é…ç½®ï¼ˆGPUã€CPUã€RAMã€å­˜å‚¨ï¼‰
2. **æ™ºèƒ½é€‰æ‹©**æœ€ä¼˜çš„æ€§èƒ½å‚æ•°
3. **åŠ¨æ€è°ƒæ•´**è¿è¡Œç­–ç•¥ä»¥å¹³è¡¡æ€§èƒ½ä¸ç¨³å®šæ€§
4. **é›¶é…ç½®**å¼€ç®±å³ç”¨ä½“éªŒï¼Œé€‚é…ä»è¯Šæ‰€ç¬”è®°æœ¬åˆ°åŒ»é™¢å·¥ä½œç«™çš„å„ç§ç¯å¢ƒ

**æ ¸å¿ƒä»·å€¼**ï¼š
- åŒ»ç”Ÿç”¨æˆ·æ— éœ€äº†è§£æŠ€æœ¯ç»†èŠ‚
- è‡ªåŠ¨æ¦¨å–ç¡¬ä»¶æ½œåŠ›ï¼Œæ€§èƒ½æå‡ 20-40%
- é«˜ç¨³å®šæ€§ä¿æŠ¤æœºåˆ¶ï¼Œé¿å…åŒ»ç–—åœºæ™¯ä¸‹çš„ä¸­æ–­
- è·¨è®¾å¤‡å…¼å®¹ï¼Œä¸€å¥—è½¯ä»¶é€‚é…æ‰€æœ‰ç¡¬ä»¶

---

## ğŸ¯ è®¾è®¡ç›®æ ‡

### ä¸»è¦ç›®æ ‡

1. **æ˜“ç”¨æ€§ä¼˜å…ˆ**
   - é»˜è®¤"Auto"æ¨¡å¼ï¼ŒåŒ»ç”Ÿç”¨æˆ·æ— éœ€é…ç½®
   - å¯åŠ¨æ—¶è‡ªåŠ¨æ£€æµ‹å¹¶æ˜¾ç¤ºç¡¬ä»¶ä¿¡æ¯
   - æä¾›ç®€å•çš„äº¤äº’ç•Œé¢é€‰æ‹©æ¨¡å¼

2. **æ€§èƒ½æœ€å¤§åŒ–**
   - æ ¹æ®ç¡¬ä»¶è‡ªåŠ¨è°ƒæ•´ï¼š
     - DataLoader workersæ•°é‡
     - GPUå†…å­˜ç®¡ç†ç­–ç•¥
     - æ‰¹å¤„ç†å¤§å°
     - æ•°æ®ä¼ è¾“ä¼˜åŒ–
   - é¢„æœŸæ€§èƒ½æå‡ï¼š20-40%ï¼ˆè§†ç¡¬ä»¶è€Œå®šï¼‰

3. **ç¨³å®šæ€§ä¿è¯**
   - OOMï¼ˆå†…å­˜æº¢å‡ºï¼‰ä¿æŠ¤æœºåˆ¶
   - GPUæ¸©åº¦ç›‘æ§ä¸è¿‡çƒ­ä¿æŠ¤
   - å¼‚å¸¸è‡ªåŠ¨é™çº§ç­–ç•¥
   - åŒ»ç–—çº§å¯é æ€§è¦æ±‚

4. **è·¨è®¾å¤‡å…¼å®¹**
   - æ”¯æŒä»4GBåˆ°24GB+çš„å„ç§GPU
   - å…¼å®¹CPUæ¨¡å¼ï¼ˆæ— GPUç¯å¢ƒï¼‰
   - Windows/Linuxè·¨å¹³å°
   - ç¬”è®°æœ¬åˆ°æœåŠ¡å™¨å…¨è¦†ç›–

### éåŠŸèƒ½æ€§ç›®æ ‡

- **å®Œå…¨ç¦»çº¿è¿è¡Œ** - ä¸éœ€è¦ç½‘ç»œè¿æ¥
- **å‘åå…¼å®¹** - ä¿ç•™æ‰‹åŠ¨é…ç½®é€‰é¡¹
- **å¯æ‰©å±•æ€§** - æ˜“äºæ·»åŠ æ–°ç¡¬ä»¶æ¡£ä½
- **å¯ç»´æŠ¤æ€§** - æ¸…æ™°çš„ä»£ç ç»“æ„å’Œæ–‡æ¡£

---

## ğŸ” å½“å‰æ€§èƒ½ç“¶é¢ˆåˆ†æ

### ç°çŠ¶è¯„ä¼°

åŸºäºä»£ç å®¡æŸ¥ï¼ˆ`core/ai_cac_inference_lib.py` å’Œ `cli/run_nb10.py`ï¼‰ï¼Œå½“å‰ç³»ç»Ÿå­˜åœ¨ä»¥ä¸‹ç“¶é¢ˆï¼š

#### 1. æ•°æ®åŠ è½½ç“¶é¢ˆï¼ˆ25-30%å¤„ç†æ—¶é—´ï¼‰

```python
# å½“å‰é…ç½® - ai_cac_inference_lib.py:136
dataloader = DataLoader(dataset, batch_size=1, shuffle=False,
                       num_workers=0,        # âŒ å•çº¿ç¨‹åŠ è½½
                       pin_memory=False)     # âŒ ç¡¬ç¼–ç ä¸ºFalse
```

**é—®é¢˜**ï¼š
- `num_workers=0` å¯¼è‡´DICOMåŠ è½½ä¸GPUæ¨ç†å®Œå…¨ä¸²è¡Œ
- å•ä¸ªcardiac CT DICOMå¯èƒ½200-400å¼ åˆ‡ç‰‡ï¼ŒåŠ è½½è€—æ—¶é•¿
- GPUæ¨ç†æ—¶CPUç©ºé—²ç­‰å¾…ï¼Œèµ„æºæµªè´¹

**å½±å“**ï¼šçº¦å æ€»å¤„ç†æ—¶é—´çš„ 25-30%

---

#### 2. CPUâ†”GPUä¼ è¾“ç“¶é¢ˆï¼ˆ10-15%å¤„ç†æ—¶é—´ï¼‰

```python
# å½“å‰ä¼ è¾“æ–¹å¼ - ai_cac_inference_lib.py:154-155
inputs = inputs.to(device)
hu_vols = hu_vols.to(device)
```

**é—®é¢˜**ï¼š
- `pin_memory=False` å¯¼è‡´æ•°æ®éœ€ç»è¿‡ä¸¤æ¬¡æ‹·è´ï¼ˆpageable â†’ pinned â†’ GPUï¼‰
- åŒæ­¥ä¼ è¾“ï¼ŒGPUç­‰å¾…æ•°æ®åˆ°è¾¾
- æœªä½¿ç”¨CUDA streamsè¿›è¡Œå¼‚æ­¥ä¼ è¾“

**å½±å“**ï¼šçº¦å æ€»å¤„ç†æ—¶é—´çš„ 10-15%

---

#### 3. GPUç¼“å­˜æ¸…ç†ç­–ç•¥ï¼ˆ5-8%å¤„ç†æ—¶é—´ï¼‰

```python
# å½“å‰æ¸…ç†ç­–ç•¥ - ai_cac_inference_lib.py:201-203
if device == 'cuda':
    del inputs, hu_vols, pred_vol
    torch.cuda.empty_cache()  # æ¯ä¸ªpatientéƒ½æ‰§è¡Œ
```

**é—®é¢˜**ï¼š
- æ¯å¤„ç†å®Œä¸€ä¸ªpatientç«‹å³æ¸…ç†GPUç¼“å­˜
- `torch.cuda.empty_cache()` æœ¬èº«æœ‰å¼€é”€ï¼ˆçº¦0.5-1ç§’ï¼‰
- å¯¹äº6GBä»¥ä¸ŠGPUï¼Œè¿‡äºä¿å®ˆ

**å½±å“**ï¼šçº¦å æ€»å¤„ç†æ—¶é—´çš„ 5-8%

---

#### 4. æ‰¹å¤„ç†å¤§å°å›ºå®šï¼ˆä¸å¯ä¼˜åŒ–ï¼‰

```python
# å›ºå®šé…ç½® - ai_cac_inference_lib.py:140
SLICE_BATCH_SIZE = 4  # å¤„ç†4ä¸ªåˆ‡ç‰‡/æ‰¹æ¬¡
```

**é—®é¢˜**ï¼š
- é’ˆå¯¹6GB GPUä¼˜åŒ–ï¼Œå¯¹äºæ›´å¤§æ˜¾å­˜æœªå……åˆ†åˆ©ç”¨
- å¯¹äº4GB GPUå¯èƒ½ä»æœ‰OOMé£é™©

**å½±å“**ï¼šæ€§èƒ½æ½œåŠ›æœªå……åˆ†å‘æŒ¥

---

### æ€§èƒ½æå‡æ½œåŠ›è¯„ä¼°

åŸºäºåŒ»å­¦å½±åƒæ·±åº¦å­¦ä¹ çš„å…¸å‹å·¥ä½œæµï¼Œå•ä¸ªPatientå¤„ç†æ—¶é—´åˆ†è§£ï¼š

```
å½“å‰åŸºçº¿ï¼ˆ100%ï¼‰:
â”œâ”€ DICOMåŠ è½½ä¸è§£æ      25-30%  â† num_workers=0 ç“¶é¢ˆ
â”œâ”€ æ•°æ®é¢„å¤„ç†ï¼ˆé‡é‡‡æ ·ï¼‰   15-20%  â† CPUæ“ä½œ
â”œâ”€ CPUâ†’GPUä¼ è¾“          10-15%  â† pin_memory=False ç“¶é¢ˆ
â”œâ”€ GPUæ¨ç†è®¡ç®—          30-40%  â† å®é™…AIè®¡ç®—ï¼ˆä¸å¯å‹ç¼©ï¼‰
â”œâ”€ GPUâ†’CPUä¼ è¾“          5-10%
â””â”€ åå¤„ç†ï¼ˆAgatstonï¼‰    5-10%

ä¼˜åŒ–åé¢„æœŸï¼ˆ60-75%ï¼‰:
â”œâ”€ DICOMåŠ è½½ï¼ˆåå°ï¼‰     0-5%    â† num_workers=2-4 éšè—åŠ è½½å»¶è¿Ÿ
â”œâ”€ æ•°æ®é¢„å¤„ç†           12-15%  â† è½»å¾®ä¼˜åŒ–
â”œâ”€ CPUâ†’GPUä¼ è¾“ï¼ˆå¼‚æ­¥ï¼‰   3-5%    â† pin_memory=True + å¼‚æ­¥
â”œâ”€ GPUæ¨ç†è®¡ç®—          30-40%  â† ä¸å˜ï¼ˆç¡¬ä»¶æé™ï¼‰
â”œâ”€ GPUâ†’CPUä¼ è¾“          4-8%
â””â”€ åå¤„ç†              5-8%
```

**ä¿å®ˆä¼°è®¡**ï¼šæ€»ä½“æ€§èƒ½æå‡ **20-30%**
**æ¿€è¿›ä¼˜åŒ–**ï¼šæ€»ä½“æ€§èƒ½æå‡ **30-40%**ï¼ˆé«˜ç«¯ç¡¬ä»¶ï¼‰

---

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     NB10 AI-CAC Application                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚           1. Hardware Detection Module                â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚ â”‚
â”‚  â”‚  â”‚ GPU Profilerâ”‚  â”‚CPU Profilerâ”‚  â”‚RAM Profilerâ”‚     â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ â”‚
â”‚  â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚ â”‚
â”‚  â”‚           â”‚Disk Profilerâ”‚                              â”‚ â”‚
â”‚  â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                         â”‚                                  â”‚
â”‚                         â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚       2. Configuration Profile Selector              â”‚ â”‚
â”‚  â”‚                                                       â”‚ â”‚
â”‚  â”‚   Hardware Metrics â†’ Scoring Algorithm â†’ Tier       â”‚ â”‚
â”‚  â”‚                                                       â”‚ â”‚
â”‚  â”‚   Tiers: Minimal | Standard | Performance |          â”‚ â”‚
â”‚  â”‚          Professional | Enterprise                    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                         â”‚                                  â”‚
â”‚                         â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚       3. Runtime Optimization Engine                 â”‚ â”‚
â”‚  â”‚                                                       â”‚ â”‚
â”‚  â”‚   â€¢ DataLoader Configuration                          â”‚ â”‚
â”‚  â”‚   â€¢ Memory Management Strategy                        â”‚ â”‚
â”‚  â”‚   â€¢ Batch Size Adjustment                             â”‚ â”‚
â”‚  â”‚   â€¢ Cache Clearing Policy                             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                         â”‚                                  â”‚
â”‚                         â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚       4. Safety & Monitoring System                  â”‚ â”‚
â”‚  â”‚                                                       â”‚ â”‚
â”‚  â”‚   â€¢ OOM Protection                                    â”‚ â”‚
â”‚  â”‚   â€¢ Temperature Monitoring                            â”‚ â”‚
â”‚  â”‚   â€¢ Auto-downgrade on Failure                         â”‚ â”‚
â”‚  â”‚   â€¢ Performance Tracking                              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                         â”‚                                  â”‚
â”‚                         â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚            5. User Interface Layer                   â”‚ â”‚
â”‚  â”‚                                                       â”‚ â”‚
â”‚  â”‚   â€¢ Hardware Detection Display                        â”‚ â”‚
â”‚  â”‚   â€¢ Mode Selection (Auto/Performance/Stable/Custom)   â”‚ â”‚
â”‚  â”‚   â€¢ Progress & Performance Metrics                    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ æ ¸å¿ƒæ¨¡å—è¯¦ç»†è®¾è®¡

### Module 1: ç¡¬ä»¶æ£€æµ‹æ¨¡å—

#### 1.1 GPUæ£€æµ‹å™¨ (GPU Profiler)

**æ–‡ä»¶ä½ç½®**: `core/hardware_profiler.py` â†’ `class GPUProfiler`

**æ£€æµ‹æŒ‡æ ‡**:

```python
class GPUInfo:
    """GPUç¡¬ä»¶ä¿¡æ¯"""
    available: bool                # æ˜¯å¦æœ‰å¯ç”¨GPU
    device_name: str               # GPUå‹å·ï¼ˆå¦‚ "NVIDIA RTX 2060"ï¼‰
    compute_capability: tuple      # CUDAè®¡ç®—èƒ½åŠ›ï¼ˆå¦‚ (7, 5)ï¼‰
    vram_total_gb: float          # æ€»æ˜¾å­˜ï¼ˆGBï¼‰
    vram_available_gb: float      # å¯ç”¨æ˜¾å­˜ï¼ˆGBï¼‰
    driver_version: str           # é©±åŠ¨ç‰ˆæœ¬
    cuda_version: str             # CUDAç‰ˆæœ¬
    supports_fp16: bool           # æ˜¯å¦æ”¯æŒFP16æ··åˆç²¾åº¦
    supports_tensor_core: bool    # æ˜¯å¦æœ‰Tensor Core
```

**æ£€æµ‹é€»è¾‘**:

```python
def detect_gpu() -> GPUInfo:
    """æ£€æµ‹GPUä¿¡æ¯"""
    if not torch.cuda.is_available():
        return GPUInfo(available=False)

    gpu_info = GPUInfo(
        available=True,
        device_name=torch.cuda.get_device_name(0),
        vram_total_gb=torch.cuda.get_device_properties(0).total_memory / 1024**3,
        vram_available_gb=(torch.cuda.get_device_properties(0).total_memory -
                          torch.cuda.memory_allocated(0)) / 1024**3,
        driver_version=torch.version.cuda,
        ...
    )

    # æµ‹è¯•å¯ç”¨æ˜¾å­˜ï¼ˆè€ƒè™‘å…¶ä»–è¿›ç¨‹å ç”¨ï¼‰
    gpu_info.vram_available_gb = measure_available_vram()

    return gpu_info
```

---

#### 1.2 CPUæ£€æµ‹å™¨ (CPU Profiler)

**æ£€æµ‹æŒ‡æ ‡**:

```python
class CPUInfo:
    """CPUç¡¬ä»¶ä¿¡æ¯"""
    physical_cores: int           # ç‰©ç†æ ¸å¿ƒæ•°
    logical_cores: int            # é€»è¾‘æ ¸å¿ƒæ•°ï¼ˆå«è¶…çº¿ç¨‹ï¼‰
    cpu_model: str                # CPUå‹å·
    frequency_mhz: float          # ä¸»é¢‘ï¼ˆMHzï¼‰
    platform: str                 # æ“ä½œç³»ç»Ÿï¼ˆWindows/Linuxï¼‰
```

**æ£€æµ‹é€»è¾‘**:

```python
import psutil
import platform

def detect_cpu() -> CPUInfo:
    """æ£€æµ‹CPUä¿¡æ¯"""
    return CPUInfo(
        physical_cores=psutil.cpu_count(logical=False),
        logical_cores=psutil.cpu_count(logical=True),
        cpu_model=platform.processor(),
        frequency_mhz=psutil.cpu_freq().max,
        platform=platform.system()
    )
```

---

#### 1.3 å†…å­˜æ£€æµ‹å™¨ (RAM Profiler)

**æ£€æµ‹æŒ‡æ ‡**:

```python
class RAMInfo:
    """å†…å­˜ä¿¡æ¯"""
    total_gb: float               # æ€»å†…å­˜ï¼ˆGBï¼‰
    available_gb: float           # å¯ç”¨å†…å­˜ï¼ˆGBï¼‰
    used_percent: float           # ä½¿ç”¨ç™¾åˆ†æ¯”
```

**æ£€æµ‹é€»è¾‘**:

```python
def detect_ram() -> RAMInfo:
    """æ£€æµ‹å†…å­˜ä¿¡æ¯"""
    mem = psutil.virtual_memory()
    return RAMInfo(
        total_gb=mem.total / 1024**3,
        available_gb=mem.available / 1024**3,
        used_percent=mem.percent
    )
```

---

#### 1.4 ç£ç›˜æ£€æµ‹å™¨ (Disk Profiler)

**æ£€æµ‹æŒ‡æ ‡**:

```python
class DiskInfo:
    """ç£ç›˜ä¿¡æ¯"""
    disk_type: str                # SSD/HDD/NVMe/Unknown
    read_speed_mbps: float        # è¯»å–é€Ÿåº¦ï¼ˆMB/sï¼Œé€šè¿‡ç®€å•benchmarkï¼‰
```

**æ£€æµ‹é€»è¾‘**:

```python
def detect_disk(data_dir: Path) -> DiskInfo:
    """æ£€æµ‹ç£ç›˜ç±»å‹å’Œæ€§èƒ½"""
    # æ–¹æ³•1: æ£€æŸ¥ç£ç›˜ç±»å‹ï¼ˆWindows: wmic, Linux: lsblkï¼‰
    disk_type = detect_disk_type(data_dir)

    # æ–¹æ³•2: ç®€å•è¯»å–benchmarkï¼ˆè¯»å–100MBæµ‹è¯•æ–‡ä»¶ï¼‰
    read_speed = benchmark_disk_read(data_dir)

    return DiskInfo(
        disk_type=disk_type,
        read_speed_mbps=read_speed
    )
```

---

### Module 2: é…ç½®æ¡£ä½ç³»ç»Ÿ

#### 2.1 é…ç½®æ¡£ä½å®šä¹‰

**æ–‡ä»¶ä½ç½®**: `core/performance_profiles.py`

å…±å®šä¹‰ **5ä¸ªé…ç½®æ¡£ä½**ï¼Œä»ä½åˆ°é«˜ï¼š

---

##### Tier 1: Minimal (æœ€å°é…ç½®)

**ç¡¬ä»¶ç‰¹å¾**:
- GPUæ˜¾å­˜: â‰¤4GB (GTX 1650, RTX 3050 4GB)
- æˆ–åªæœ‰CPU

**é…ç½®å‚æ•°**:

```yaml
profile_minimal:
  device: "cuda"  # æˆ– "cpu" (è‹¥æ— GPU)
  num_workers: 0
  pin_memory: false
  slice_batch_size: 2
  clear_cache_interval: 1
  use_mixed_precision: false
  prefetch_next_patient: false
  async_data_transfer: false
```

**é¢„æœŸæ€§èƒ½**:
- å•ä¾‹å¤„ç†æ—¶é—´: 45-60ç§’
- ç¨³å®šæ€§: â˜…â˜…â˜…â˜…â˜…
- é€‚ç”¨åœºæ™¯: è¯Šæ‰€ç¬”è®°æœ¬ã€ä½ç«¯å·¥ä½œç«™

---

##### Tier 2: Standard (æ ‡å‡†é…ç½®) â† **å½“å‰RTX 2060åœºæ™¯**

**ç¡¬ä»¶ç‰¹å¾**:
- GPUæ˜¾å­˜: 6GB (RTX 2060, GTX 1060 6GB)
- RAM: 16GB+
- CPU: 4æ ¸+

**é…ç½®å‚æ•°**:

```yaml
profile_standard:
  device: "cuda"
  num_workers: 2                    # Windowsä¿å®ˆå€¼
  pin_memory: true                  # âœ… ä¿®å¤å½“å‰ç¡¬ç¼–ç é—®é¢˜
  slice_batch_size: 4
  clear_cache_interval: 1
  use_mixed_precision: false
  prefetch_next_patient: false
  async_data_transfer: false        # å¯é€‰ï¼štrueï¼ˆä¸­ç­‰é£é™©ï¼‰
```

**é¢„æœŸæ€§èƒ½**:
- å•ä¾‹å¤„ç†æ—¶é—´: 28-32ç§’ï¼ˆâ†“18-20% vs å½“å‰ï¼‰
- ç¨³å®šæ€§: â˜…â˜…â˜…â˜…â˜†
- é€‚ç”¨åœºæ™¯: æ ‡å‡†åŒ»ç–—å·¥ä½œç«™

**æ€§èƒ½æå‡æ¥æº**:
- `num_workers=2`: å‡å°‘DICOMåŠ è½½ç­‰å¾… (+10-12%)
- `pin_memory=true`: åŠ é€ŸCPUâ†’GPUä¼ è¾“ (+5-8%)
- æ€»è®¡: **+18-20%**

---

##### Tier 3: Performance (é«˜æ€§èƒ½é…ç½®)

**ç¡¬ä»¶ç‰¹å¾**:
- GPUæ˜¾å­˜: 8-12GB (RTX 3060 12GB, RTX 3070, RTX 4060 Ti)
- RAM: 32GB+
- CPU: 6æ ¸+

**é…ç½®å‚æ•°**:

```yaml
profile_performance:
  device: "cuda"
  num_workers: 4
  pin_memory: true
  slice_batch_size: 6
  clear_cache_interval: 3
  use_mixed_precision: true         # å¯ç”¨FP16åŠ é€Ÿ
  prefetch_next_patient: true       # é¢„åŠ è½½ä¸‹ä¸€ä¸ªpatient
  async_data_transfer: true         # CUDA streamså¼‚æ­¥ä¼ è¾“
  dataloader_prefetch_factor: 2
```

**é¢„æœŸæ€§èƒ½**:
- å•ä¾‹å¤„ç†æ—¶é—´: 18-22ç§’ï¼ˆâ†“50% vs å½“å‰ï¼‰
- ç¨³å®šæ€§: â˜…â˜…â˜…â˜…â˜†
- é€‚ç”¨åœºæ™¯: å¤§å‹åŒ»é™¢å½±åƒä¸­å¿ƒ

---

##### Tier 4: Professional (ä¸“ä¸šé…ç½®)

**ç¡¬ä»¶ç‰¹å¾**:
- GPUæ˜¾å­˜: 16-24GB (RTX 4080, RTX A5000, RTX 4090)
- RAM: 64GB+
- CPU: 8æ ¸+
- å­˜å‚¨: NVMe SSD

**é…ç½®å‚æ•°**:

```yaml
profile_professional:
  device: "cuda"
  num_workers: 6
  pin_memory: true
  slice_batch_size: 8
  clear_cache_interval: 5
  use_mixed_precision: true
  prefetch_next_patient: true
  async_data_transfer: true
  dataloader_prefetch_factor: 3
```

**é¢„æœŸæ€§èƒ½**:
- å•ä¾‹å¤„ç†æ—¶é—´: 12-15ç§’ï¼ˆâ†“65% vs å½“å‰ï¼‰
- ç¨³å®šæ€§: â˜…â˜…â˜…â˜…â˜†
- é€‚ç”¨åœºæ™¯: ç§‘ç ”æœºæ„ã€ä¸‰ç”²åŒ»é™¢AIä¸­å¿ƒ

---

##### Tier 5: Enterprise (æœåŠ¡å™¨é…ç½®)

**ç¡¬ä»¶ç‰¹å¾**:
- GPU: å¤šå¡æˆ–A100/H100
- RAM: 128GB+
- å­˜å‚¨: RAID SSDé˜µåˆ—

**é…ç½®å‚æ•°**:

```yaml
profile_enterprise:
  device: "cuda"
  multi_gpu: true                   # å¤šGPUå¹¶è¡Œ
  num_workers: 16
  pin_memory: true
  slice_batch_size: 12
  clear_cache_interval: 10
  use_mixed_precision: true
  prefetch_next_patient: true
  async_data_transfer: true
  dataloader_prefetch_factor: 4
```

**é¢„æœŸæ€§èƒ½**:
- å•ä¾‹å¤„ç†æ—¶é—´: 8-10ç§’
- ååé‡: å¯å¹¶è¡Œå¤„ç†å¤šä¸ªpatient
- é€‚ç”¨åœºæ™¯: äº‘ç«¯AIæœåŠ¡ã€å¤§è§„æ¨¡ç­›æŸ¥

---

#### 2.2 æ¡£ä½é€‰æ‹©ç®—æ³•

**æ–‡ä»¶ä½ç½®**: `core/performance_profiles.py` â†’ `select_optimal_profile()`

**è¯„åˆ†ç®—æ³•**:

```python
def select_optimal_profile(
    gpu_info: GPUInfo,
    cpu_info: CPUInfo,
    ram_info: RAMInfo,
    disk_info: DiskInfo
) -> ProfileTier:
    """
    æ ¹æ®ç¡¬ä»¶ä¿¡æ¯é€‰æ‹©æœ€ä¼˜é…ç½®æ¡£ä½

    è¯„åˆ†æƒé‡ï¼š
    - GPU: 60%
    - RAM: 20%
    - CPU: 15%
    - Disk: 5%
    """
    score = 0

    # 1. GPUè¯„åˆ†ï¼ˆ60åˆ†ï¼‰
    if not gpu_info.available:
        score += 0  # CPUæ¨¡å¼
    elif gpu_info.vram_total_gb >= 16:
        score += 60
    elif gpu_info.vram_total_gb >= 8:
        score += 45
    elif gpu_info.vram_total_gb >= 6:
        score += 30
    elif gpu_info.vram_total_gb >= 4:
        score += 15
    else:
        score += 5

    # 2. RAMè¯„åˆ†ï¼ˆ20åˆ†ï¼‰
    if ram_info.total_gb >= 64:
        score += 20
    elif ram_info.total_gb >= 32:
        score += 15
    elif ram_info.total_gb >= 16:
        score += 10
    else:
        score += 5

    # 3. CPUè¯„åˆ†ï¼ˆ15åˆ†ï¼‰
    if cpu_info.physical_cores >= 8:
        score += 15
    elif cpu_info.physical_cores >= 6:
        score += 11
    elif cpu_info.physical_cores >= 4:
        score += 7
    else:
        score += 3

    # 4. ç£ç›˜è¯„åˆ†ï¼ˆ5åˆ†ï¼‰
    if disk_info.disk_type == "NVMe":
        score += 5
    elif disk_info.disk_type == "SSD":
        score += 3
    else:
        score += 1

    # æ˜ å°„åˆ°æ¡£ä½
    if score >= 85:
        return ProfileTier.ENTERPRISE
    elif score >= 65:
        return ProfileTier.PROFESSIONAL
    elif score >= 45:
        return ProfileTier.PERFORMANCE
    elif score >= 25:
        return ProfileTier.STANDARD
    else:
        return ProfileTier.MINIMAL
```

**ç¤ºä¾‹è¯„åˆ†**:

| ç¡¬ä»¶é…ç½® | GPU | RAM | CPU | Disk | æ€»åˆ† | æ¡£ä½ |
|---------|-----|-----|-----|------|------|------|
| RTX 2060 + 16GB + i5 6æ ¸ + SSD | 30 | 10 | 11 | 3 | **54** | Standard |
| RTX 3060 12GB + 32GB + i7 8æ ¸ + NVMe | 45 | 15 | 15 | 5 | **80** | Professional |
| RTX 4090 + 64GB + i9 12æ ¸ + NVMe | 60 | 20 | 15 | 5 | **100** | Enterprise |
| CPU only + 8GB | 0 | 5 | 7 | 1 | **13** | Minimal |

---

### Module 3: è¿è¡Œæ—¶ä¼˜åŒ–å¼•æ“

#### 3.1 DataLoaderé…ç½®ä¼˜åŒ–

**ä¿®æ”¹æ–‡ä»¶**: `core/ai_cac_inference_lib.py:135-136`

**å½“å‰ä»£ç **:
```python
dataloader = DataLoader(dataset, batch_size=1, shuffle=False,
                       num_workers=0, pin_memory=False)
```

**ä¼˜åŒ–åä»£ç **:
```python
# ä»é…ç½®ä¸­è¯»å–ä¼˜åŒ–å‚æ•°
from core.performance_profiles import get_active_profile

profile = get_active_profile()

dataloader = DataLoader(
    dataset,
    batch_size=1,
    shuffle=False,
    num_workers=profile.num_workers,           # åŠ¨æ€å€¼ï¼š0-16
    pin_memory=profile.pin_memory,             # åŠ¨æ€å€¼ï¼šTrue/False
    prefetch_factor=profile.prefetch_factor if profile.num_workers > 0 else None,
    persistent_workers=profile.num_workers > 0 # å¤ç”¨workerè¿›ç¨‹
)
```

---

#### 3.2 GPUç¼“å­˜æ¸…ç†ç­–ç•¥ä¼˜åŒ–

**ä¿®æ”¹æ–‡ä»¶**: `core/ai_cac_inference_lib.py:201-203`

**å½“å‰ä»£ç **:
```python
if device == 'cuda':
    del inputs, hu_vols, pred_vol
    torch.cuda.empty_cache()  # æ¯æ¬¡éƒ½æ¸…ç†
```

**ä¼˜åŒ–åä»£ç **:
```python
# æ ¹æ®é…ç½®åŠ¨æ€æ¸…ç†
patient_counter += 1

# æ¸…ç†ä¸´æ—¶å˜é‡
del inputs, hu_vols, pred_vol

# æ ¹æ®é…ç½®é—´éš”æ¸…ç†GPUç¼“å­˜
if device == 'cuda' and patient_counter % profile.clear_cache_interval == 0:
    torch.cuda.empty_cache()
    logger.debug(f"GPU cache cleared (interval={profile.clear_cache_interval})")
```

---

#### 3.3 å¼‚æ­¥æ•°æ®ä¼ è¾“ä¼˜åŒ–ï¼ˆé«˜çº§åŠŸèƒ½ï¼‰

**æ–°å¢ä»£ç **: `core/ai_cac_inference_lib.py` â†’ `run_inference_on_dicom_folder()`

**å®ç°CUDA streamså¼‚æ­¥ä¼ è¾“**:

```python
if profile.async_data_transfer and device == 'cuda':
    # åˆ›å»ºå¼‚æ­¥ä¼ è¾“stream
    stream = torch.cuda.Stream()

    with torch.cuda.stream(stream):
        inputs = inputs.to(device, non_blocking=True)
        hu_vols = hu_vols.to(device, non_blocking=True)

    # GPUå¯ä»¥åŒæ—¶æ‰§è¡Œå…¶ä»–æ“ä½œï¼Œä¼ è¾“å®Œæˆå‰ç­‰å¾…
    torch.cuda.current_stream().wait_stream(stream)
else:
    # åŒæ­¥ä¼ è¾“ï¼ˆé»˜è®¤ï¼‰
    inputs = inputs.to(device)
    hu_vols = hu_vols.to(device)
```

---

#### 3.4 æ··åˆç²¾åº¦æ¨ç†ï¼ˆé«˜çº§åŠŸèƒ½ï¼‰

**ä¿®æ”¹æ–‡ä»¶**: `core/ai_cac_inference_lib.py` â†’ æ¨ç†å¾ªç¯

**æ·»åŠ FP16æ”¯æŒ**:

```python
if profile.use_mixed_precision and device == 'cuda':
    # ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦
    with torch.cuda.amp.autocast():
        batch_out = model(batch.float())
else:
    # æ ‡å‡†FP32æ¨ç†
    batch_out = model(batch.float())
```

**æ³¨æ„äº‹é¡¹**:
- éœ€è¦éªŒè¯æ··åˆç²¾åº¦å¯¹Agatston scoreè®¡ç®—ç²¾åº¦çš„å½±å“
- å»ºè®®åœ¨ProfessionalåŠä»¥ä¸Šæ¡£ä½å¯ç”¨
- éœ€è¦åœ¨æ–‡æ¡£ä¸­è¯´æ˜ç²¾åº¦tradeoff

---

### Module 4: å®‰å…¨ä¸ç›‘æ§ç³»ç»Ÿ

#### 4.1 OOMï¼ˆå†…å­˜æº¢å‡ºï¼‰ä¿æŠ¤

**æ–‡ä»¶ä½ç½®**: `core/safety_monitor.py` â†’ `class OOMProtector`

**ä¿æŠ¤ç­–ç•¥**:

```python
class OOMProtector:
    """OOMä¿æŠ¤å™¨"""

    def __init__(self, profile: PerformanceProfile):
        self.profile = profile
        self.oom_count = 0
        self.max_oom_retry = 3

    def pre_inference_check(self) -> bool:
        """æ¨ç†å‰æ£€æŸ¥GPUå†…å­˜"""
        if not torch.cuda.is_available():
            return True

        # æ£€æŸ¥å¯ç”¨æ˜¾å­˜
        available = torch.cuda.memory_available() / 1024**3
        required = self.estimate_memory_requirement()

        if available < required * 1.2:  # éœ€è¦20%å®‰å…¨è¾¹é™…
            logger.warning(f"å¯ç”¨æ˜¾å­˜ä¸è¶³: {available:.1f}GB < {required*1.2:.1f}GB")
            logger.warning("å»ºè®®é™ä½SLICE_BATCH_SIZEæˆ–åˆ‡æ¢åˆ°ç¨³å®šæ¨¡å¼")
            return False

        return True

    def handle_oom_exception(self, e: Exception) -> bool:
        """å¤„ç†OOMå¼‚å¸¸"""
        self.oom_count += 1

        if self.oom_count >= self.max_oom_retry:
            logger.error("è¿ç»­3æ¬¡OOMï¼Œå»ºè®®åˆ‡æ¢åˆ°CPUæ¨¡å¼æˆ–ç¨³å®šæ¨¡å¼")
            return False

        # è‡ªåŠ¨é™çº§ç­–ç•¥
        logger.warning(f"æ£€æµ‹åˆ°OOM (ç¬¬{self.oom_count}æ¬¡)ï¼Œè‡ªåŠ¨é™çº§...")

        # é™ä½SLICE_BATCH_SIZE
        self.profile.slice_batch_size = max(2, self.profile.slice_batch_size - 2)

        # æ¸…ç†GPUç¼“å­˜
        torch.cuda.empty_cache()

        logger.info(f"å·²é™çº§: SLICE_BATCH_SIZE={self.profile.slice_batch_size}")
        return True  # å¯ä»¥é‡è¯•
```

---

#### 4.2 GPUæ¸©åº¦ç›‘æ§

**æ–‡ä»¶ä½ç½®**: `core/safety_monitor.py` â†’ `class TemperatureMonitor`

**ç›‘æ§ç­–ç•¥**:

```python
class TemperatureMonitor:
    """GPUæ¸©åº¦ç›‘æ§å™¨"""

    def __init__(self, max_temp: float = 85.0, critical_temp: float = 90.0):
        self.max_temp = max_temp
        self.critical_temp = critical_temp
        self.high_temp_count = 0

    def check_temperature(self) -> str:
        """
        æ£€æŸ¥GPUæ¸©åº¦

        Returns:
            "ok" | "warning" | "critical"
        """
        try:
            # ä½¿ç”¨nvidia-smiæˆ–pynvmlè¯»å–æ¸©åº¦
            temp = self.get_gpu_temperature()

            if temp >= self.critical_temp:
                logger.error(f"GPUæ¸©åº¦è¿‡é«˜: {temp}Â°C >= {self.critical_temp}Â°C")
                return "critical"
            elif temp >= self.max_temp:
                self.high_temp_count += 1
                logger.warning(f"GPUæ¸©åº¦åé«˜: {temp}Â°C (è­¦å‘Šé˜ˆå€¼: {self.max_temp}Â°C)")

                if self.high_temp_count >= 3:
                    logger.warning("GPUæ¸©åº¦æŒç»­åé«˜ï¼Œæš‚åœ10ç§’é™æ¸©...")
                    time.sleep(10)
                    self.high_temp_count = 0

                return "warning"
            else:
                self.high_temp_count = 0
                return "ok"

        except Exception as e:
            logger.debug(f"æ— æ³•è¯»å–GPUæ¸©åº¦: {e}")
            return "ok"

    def get_gpu_temperature(self) -> float:
        """è·å–GPUæ¸©åº¦ï¼ˆå•ä½ï¼šæ‘„æ°åº¦ï¼‰"""
        # æ–¹æ³•1: ä½¿ç”¨pynvml (æ¨è)
        try:
            import pynvml
            pynvml.nvmlInit()
            handle = pynvml.nvmlDeviceGetHandleByIndex(0)
            temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
            pynvml.nvmlShutdown()
            return float(temp)
        except:
            pass

        # æ–¹æ³•2: è§£ænvidia-smiè¾“å‡º
        try:
            import subprocess
            result = subprocess.run(
                ['nvidia-smi', '--query-gpu=temperature.gpu', '--format=csv,noheader,nounits'],
                capture_output=True, text=True, timeout=5
            )
            return float(result.stdout.strip())
        except:
            pass

        # æ— æ³•è·å–æ¸©åº¦
        raise RuntimeError("æ— æ³•è¯»å–GPUæ¸©åº¦")
```

---

#### 4.3 æ€§èƒ½è·Ÿè¸ªä¸å¼‚å¸¸æ£€æµ‹

**æ–‡ä»¶ä½ç½®**: `core/safety_monitor.py` â†’ `class PerformanceTracker`

**è·Ÿè¸ªæŒ‡æ ‡**:

```python
class PerformanceTracker:
    """æ€§èƒ½è·Ÿè¸ªå™¨"""

    def __init__(self, expected_time_per_patient: float):
        self.expected_time = expected_time_per_patient
        self.processing_times = []
        self.check_interval = 5  # æ¯5ä¸ªpatientæ£€æŸ¥ä¸€æ¬¡

    def record_time(self, patient_id: str, elapsed_time: float):
        """è®°å½•å¤„ç†æ—¶é—´"""
        self.processing_times.append(elapsed_time)

        if len(self.processing_times) % self.check_interval == 0:
            self.check_performance()

    def check_performance(self):
        """æ£€æŸ¥æ€§èƒ½æ˜¯å¦æ­£å¸¸"""
        if len(self.processing_times) < self.check_interval:
            return

        # è®¡ç®—æœ€è¿‘5ä¸ªpatientçš„å¹³å‡æ—¶é—´
        recent_avg = np.mean(self.processing_times[-self.check_interval:])

        # å¦‚æœå®é™…æ—¶é—´ > é¢„æœŸæ—¶é—´çš„1.5å€
        if recent_avg > self.expected_time * 1.5:
            logger.warning("="*60)
            logger.warning(f"âš ï¸ æ€§èƒ½ä½äºé¢„æœŸ")
            logger.warning(f"   é¢„æœŸæ—¶é—´: {self.expected_time:.1f}ç§’/ä¾‹")
            logger.warning(f"   å®é™…æ—¶é—´: {recent_avg:.1f}ç§’/ä¾‹ (æ…¢ {(recent_avg/self.expected_time-1)*100:.0f}%)")
            logger.warning("")
            logger.warning("å¯èƒ½åŸå› :")
            logger.warning("  1. å…¶ä»–ç¨‹åºå ç”¨GPU/CPUèµ„æº")
            logger.warning("  2. ç£ç›˜IOç“¶é¢ˆï¼ˆDICOMæ–‡ä»¶è¯»å–æ…¢ï¼‰")
            logger.warning("  3. ç³»ç»Ÿå†…å­˜ä¸è¶³")
            logger.warning("")
            logger.warning("å»ºè®®æ“ä½œ:")
            logger.warning("  1. å…³é—­å…¶ä»–ç¨‹åºé‡Šæ”¾èµ„æº")
            logger.warning("  2. åˆ‡æ¢åˆ°'Stable'ç¨³å®šæ¨¡å¼")
            logger.warning("="*60)
```

---

#### 4.4 è‡ªåŠ¨é™çº§ç­–ç•¥

**æ–‡ä»¶ä½ç½®**: `core/safety_monitor.py` â†’ `class AutoDowngradeManager`

**é™çº§é€»è¾‘**:

```python
class AutoDowngradeManager:
    """è‡ªåŠ¨é™çº§ç®¡ç†å™¨"""

    def __init__(self, current_profile: ProfileTier):
        self.current_tier = current_profile
        self.failure_count = 0
        self.max_failures = 3

    def record_failure(self, error_type: str):
        """è®°å½•å¤±è´¥"""
        self.failure_count += 1
        logger.warning(f"è®°å½•å¤±è´¥äº‹ä»¶: {error_type} (ç´¯è®¡: {self.failure_count}/{self.max_failures})")

        if self.failure_count >= self.max_failures:
            self.trigger_downgrade()

    def trigger_downgrade(self):
        """è§¦å‘é™çº§"""
        logger.warning("="*60)
        logger.warning("âš ï¸ è¿ç»­å¤±è´¥è¶…è¿‡é˜ˆå€¼ï¼Œè§¦å‘è‡ªåŠ¨é™çº§")
        logger.warning("")

        # é™çº§åˆ°ä¸‹ä¸€æ¡£ä½
        downgrade_map = {
            ProfileTier.ENTERPRISE: ProfileTier.PROFESSIONAL,
            ProfileTier.PROFESSIONAL: ProfileTier.PERFORMANCE,
            ProfileTier.PERFORMANCE: ProfileTier.STANDARD,
            ProfileTier.STANDARD: ProfileTier.MINIMAL,
            ProfileTier.MINIMAL: None  # æ— æ³•é™çº§
        }

        new_tier = downgrade_map.get(self.current_tier)

        if new_tier is None:
            logger.error("å·²å¤„äºæœ€ä½æ¡£ä½ï¼Œæ— æ³•ç»§ç»­é™çº§")
            logger.error("å»ºè®®åˆ‡æ¢åˆ°CPUæ¨¡å¼æˆ–æ£€æŸ¥ç¡¬ä»¶")
            return False

        logger.warning(f"æ¡£ä½åˆ‡æ¢: {self.current_tier.name} â†’ {new_tier.name}")
        logger.warning("="*60)

        # åº”ç”¨æ–°é…ç½®
        apply_profile(new_tier)
        self.current_tier = new_tier
        self.failure_count = 0  # é‡ç½®å¤±è´¥è®¡æ•°

        return True
```

---

### Module 5: ç”¨æˆ·ç•Œé¢å±‚

#### 5.1 å¯åŠ¨æ—¶ç¡¬ä»¶æ£€æµ‹æ˜¾ç¤º

**ä¿®æ”¹æ–‡ä»¶**: `cli/run_nb10.py` â†’ `main()`

**äº¤äº’ç•Œé¢è®¾è®¡**:

```python
def display_hardware_detection_and_select_mode(
    gpu_info: GPUInfo,
    cpu_info: CPUInfo,
    ram_info: RAMInfo,
    disk_info: DiskInfo,
    recommended_tier: ProfileTier
) -> ProfileTier:
    """
    æ˜¾ç¤ºç¡¬ä»¶æ£€æµ‹ç»“æœå¹¶è®©ç”¨æˆ·é€‰æ‹©æ¨¡å¼

    Returns:
        ç”¨æˆ·é€‰æ‹©çš„é…ç½®æ¡£ä½
    """
    print("="*70)
    print("NB10 AI-CAC æ™ºèƒ½ç¡¬ä»¶æ£€æµ‹")
    print("="*70)
    print()
    print("æ­£åœ¨æ£€æµ‹ç¡¬ä»¶é…ç½®...")
    print()

    # æ˜¾ç¤ºç¡¬ä»¶ä¿¡æ¯
    if gpu_info.available:
        print(f"âœ“ GPU: {gpu_info.device_name} ({gpu_info.vram_total_gb:.1f}GB)")
    else:
        print(f"âš  GPU: ä¸å¯ç”¨ (å°†ä½¿ç”¨CPUæ¨¡å¼)")

    print(f"âœ“ RAM: {ram_info.total_gb:.1f}GB (å¯ç”¨: {ram_info.available_gb:.1f}GB)")
    print(f"âœ“ CPU: {cpu_info.cpu_model} ({cpu_info.physical_cores}æ ¸{cpu_info.logical_cores}çº¿ç¨‹)")
    print(f"âœ“ ç£ç›˜: {disk_info.disk_type}")
    print()

    print("-"*70)
    print(f"æ¨èé…ç½®æ¡£ä½: {recommended_tier.name.title()}")
    print()

    # æ˜¾ç¤ºé¢„æœŸæ€§èƒ½
    performance = get_expected_performance(recommended_tier)
    print(f"é¢„è®¡å•ä¾‹å¤„ç†æ—¶é—´: {performance.time_per_patient_sec:.0f}ç§’")

    # å¦‚æœç”¨æˆ·æœ‰æŒ‡å®šç—…ä¾‹æ•°ï¼Œè®¡ç®—æ€»è€—æ—¶
    # print(f"é¢„è®¡æ€»è€—æ—¶(197ä¾‹): {performance.time_per_patient_sec * 197 / 3600:.1f}å°æ—¶")

    print("-"*70)
    print()

    # ç”¨æˆ·é€‰æ‹©
    print("å¯é€‰æ¨¡å¼:")
    print("  [1] Auto (æ¨è) - è‡ªåŠ¨é€‰æ‹©æœ€ä½³é…ç½®")
    print("  [2] Performance - é«˜æ€§èƒ½æ¨¡å¼ (å¯èƒ½ä¸ç¨³å®š)")
    print("  [3] Stable - ç¨³å®šæ¨¡å¼ (é€Ÿåº¦è¾ƒæ…¢ï¼Œç¨³å®šæ€§æœ€é«˜)")
    print("  [4] Custom - æ‰‹åŠ¨é…ç½®")
    print()

    while True:
        choice = input("è¯·é€‰æ‹© [1-4] (ç›´æ¥å›è½¦ä½¿ç”¨æ¨è): ").strip()

        if choice == "" or choice == "1":
            print(f"\nâœ“ å·²é€‰æ‹©: Auto - {recommended_tier.name.title()} æ¨¡å¼")
            return recommended_tier

        elif choice == "2":
            # å°è¯•å‡çº§åˆ°Performanceæ¡£ä½
            if recommended_tier.value < ProfileTier.PERFORMANCE.value:
                print("\nâš ï¸ è­¦å‘Š: å½“å‰ç¡¬ä»¶é…ç½®ä½äºPerformanceæ¨¡å¼è¦æ±‚")
                confirm = input("   å¼ºåˆ¶ä½¿ç”¨å¯èƒ½å¯¼è‡´ä¸ç¨³å®šï¼Œæ˜¯å¦ç»§ç»­? (yes/no): ")
                if confirm.lower() == 'yes':
                    print("\nâœ“ å·²é€‰æ‹©: Performance é«˜æ€§èƒ½æ¨¡å¼")
                    return ProfileTier.PERFORMANCE
                else:
                    continue
            else:
                print("\nâœ“ å·²é€‰æ‹©: Performance é«˜æ€§èƒ½æ¨¡å¼")
                return ProfileTier.PERFORMANCE

        elif choice == "3":
            # é™çº§åˆ°Minimal/Standard
            stable_tier = ProfileTier.STANDARD if recommended_tier.value >= ProfileTier.STANDARD.value else ProfileTier.MINIMAL
            print(f"\nâœ“ å·²é€‰æ‹©: Stable - {stable_tier.name.title()} ç¨³å®šæ¨¡å¼")
            return stable_tier

        elif choice == "4":
            print("\nè‡ªå®šä¹‰æ¨¡å¼æš‚æœªå®ç°ï¼Œè¯·ç¼–è¾‘ config.yaml æ–‡ä»¶")
            print("å°†ä½¿ç”¨æ¨èé…ç½®...")
            return recommended_tier

        else:
            print("âš ï¸ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1-4")
```

**è¾“å‡ºç¤ºä¾‹ï¼ˆRTX 2060ç¯å¢ƒï¼‰**:

```
======================================================================
NB10 AI-CAC æ™ºèƒ½ç¡¬ä»¶æ£€æµ‹
======================================================================

æ­£åœ¨æ£€æµ‹ç¡¬ä»¶é…ç½®...

âœ“ GPU: NVIDIA GeForce RTX 2060 (6.0GB)
âœ“ RAM: 16.0GB (å¯ç”¨: 10.2GB)
âœ“ CPU: Intel(R) Core(TM) i5-10400 (6æ ¸12çº¿ç¨‹)
âœ“ ç£ç›˜: SSD

----------------------------------------------------------------------
æ¨èé…ç½®æ¡£ä½: Standard

é¢„è®¡å•ä¾‹å¤„ç†æ—¶é—´: 30ç§’
----------------------------------------------------------------------

å¯é€‰æ¨¡å¼:
  [1] Auto (æ¨è) - è‡ªåŠ¨é€‰æ‹©æœ€ä½³é…ç½®
  [2] Performance - é«˜æ€§èƒ½æ¨¡å¼ (å¯èƒ½ä¸ç¨³å®š)
  [3] Stable - ç¨³å®šæ¨¡å¼ (é€Ÿåº¦è¾ƒæ…¢ï¼Œç¨³å®šæ€§æœ€é«˜)
  [4] Custom - æ‰‹åŠ¨é…ç½®

è¯·é€‰æ‹© [1-4] (ç›´æ¥å›è½¦ä½¿ç”¨æ¨è): _
```

---

#### 5.2 è¿è¡Œæ—¶æ€§èƒ½ç›‘æ§æ˜¾ç¤º

**ä¿®æ”¹æ–‡ä»¶**: `cli/run_nb10.py` â†’ `run_inference_batch()`

**å®æ—¶æ˜¾ç¤º**:

```python
def run_inference_batch(...):
    """è¿è¡Œæ‰¹é‡æ¨ç†ï¼ˆå¸¦æ€§èƒ½ç›‘æ§ï¼‰"""

    # åˆå§‹åŒ–ç›‘æ§å™¨
    perf_tracker = PerformanceTracker(expected_time=profile.expected_time_per_patient)
    temp_monitor = TemperatureMonitor()

    for i, folder_path in enumerate(dicom_folders, 1):
        patient_id = folder_path.name

        # æ˜¾ç¤ºè¿›åº¦
        logger.info(f"[{i}/{len(dicom_folders)}] Processing: {patient_id}")

        # æ¸©åº¦æ£€æŸ¥
        temp_status = temp_monitor.check_temperature()
        if temp_status == "critical":
            logger.error("GPUæ¸©åº¦è¿‡é«˜ï¼Œå¼ºåˆ¶åˆ‡æ¢åˆ°CPUæ¨¡å¼")
            config.set('processing.device', 'cpu')

        start_time = time.time()

        try:
            result = run_inference_on_dicom_folder(...)
            elapsed = time.time() - start_time

            # è®°å½•æ€§èƒ½
            perf_tracker.record_time(patient_id, elapsed)

            # æ˜¾ç¤ºç»“æœï¼ˆå¸¦æ—¶é—´ï¼‰
            logger.info(f"  âœ“ Success - Agatston: {result['agatston_score']:.2f} "
                       f"(è€—æ—¶: {elapsed:.1f}ç§’)")

        except Exception as e:
            logger.error(f"  âœ— Failed - {str(e)}")
            # è®°å½•å¤±è´¥ï¼Œè§¦å‘è‡ªåŠ¨é™çº§æ£€æŸ¥
            auto_downgrade.record_failure(str(e))
```

---

#### 5.3 å®Œæˆåæ€§èƒ½æ€»ç»“

**æ·»åŠ åŠŸèƒ½**: åœ¨å¤„ç†å®Œæˆåæ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡

```python
def display_performance_summary(results_df: pd.DataFrame, profile: PerformanceProfile):
    """æ˜¾ç¤ºæ€§èƒ½æ€»ç»“"""

    success_df = results_df[results_df['status'] == 'success']

    if len(success_df) == 0:
        return

    print()
    print("="*70)
    print("æ€§èƒ½æ€»ç»“")
    print("="*70)
    print()
    print(f"é…ç½®æ¡£ä½: {profile.tier.name.title()}")
    print(f"æˆåŠŸå¤„ç†: {len(success_df)} ä¾‹")
    print()

    # å‡è®¾æˆ‘ä»¬åœ¨æ¨ç†æ—¶è®°å½•äº†æ¯ä¾‹çš„å¤„ç†æ—¶é—´ï¼ˆéœ€è¦æ·»åŠ æ­¤å­—æ®µï¼‰
    if 'processing_time_sec' in success_df.columns:
        avg_time = success_df['processing_time_sec'].mean()
        min_time = success_df['processing_time_sec'].min()
        max_time = success_df['processing_time_sec'].max()

        print(f"å¹³å‡å¤„ç†æ—¶é—´: {avg_time:.1f}ç§’/ä¾‹")
        print(f"æœ€å¿«: {min_time:.1f}ç§’  |  æœ€æ…¢: {max_time:.1f}ç§’")
        print()

        # ä¸é¢„æœŸæ€§èƒ½å¯¹æ¯”
        expected = profile.expected_time_per_patient
        diff_percent = (avg_time - expected) / expected * 100

        if abs(diff_percent) < 10:
            print(f"âœ“ æ€§èƒ½ç¬¦åˆé¢„æœŸ (é¢„æœŸ: {expected:.1f}ç§’)")
        elif diff_percent > 0:
            print(f"âš ï¸ æ€§èƒ½ä½äºé¢„æœŸ {diff_percent:.0f}% (é¢„æœŸ: {expected:.1f}ç§’)")
            print("   å»ºè®®æ£€æŸ¥ç³»ç»Ÿèµ„æºå ç”¨æˆ–åˆ‡æ¢åˆ°ç¨³å®šæ¨¡å¼")
        else:
            print(f"âœ“ æ€§èƒ½è¶…å‡ºé¢„æœŸ {-diff_percent:.0f}% (é¢„æœŸ: {expected:.1f}ç§’)")

    print("="*70)
```

---

## ğŸ“ é…ç½®æ–‡ä»¶å¢å¼º

### æ›´æ–° config.yaml ç»“æ„

**æ–‡ä»¶ä½ç½®**: `config/config.yaml` å’Œ `config/config.yaml.template`

**æ–°å¢é…ç½®æ®µ**:

```yaml
# ============================================================
# Hardware Detection & Performance Configuration
# ============================================================
performance:
  # ç¡¬ä»¶æ£€æµ‹é…ç½®
  hardware_detection:
    enabled: true                    # å¯ç”¨è‡ªåŠ¨ç¡¬ä»¶æ£€æµ‹
    mode: "auto"                     # auto/manual
    profile: "standard"              # æ‰‹åŠ¨æŒ‡å®šæ¡£ä½ï¼ˆä»…åœ¨mode=manualæ—¶ç”Ÿæ•ˆï¼‰
                                     # å¯é€‰: minimal/standard/performance/professional/enterprise

  # è¿è¡Œæ—¶å®‰å…¨ä¿æŠ¤
  safety:
    oom_protection: true             # OOMä¿æŠ¤
    temperature_monitoring: true     # GPUæ¸©åº¦ç›‘æ§
    max_gpu_temp: 85.0               # GPUæ¸©åº¦è­¦å‘Šé˜ˆå€¼ï¼ˆæ‘„æ°åº¦ï¼‰
    critical_gpu_temp: 90.0          # GPUæ¸©åº¦ç´§æ€¥é˜ˆå€¼
    memory_leak_detection: true      # å†…å­˜æ³„æ¼æ£€æµ‹
    auto_downgrade: true             # å¼‚å¸¸æ—¶è‡ªåŠ¨é™çº§

  # æ€§èƒ½è·Ÿè¸ª
  tracking:
    enabled: true                    # å¯ç”¨æ€§èƒ½è·Ÿè¸ª
    check_interval: 5                # æ¯Nä¸ªpatientæ£€æŸ¥ä¸€æ¬¡æ€§èƒ½
    save_metrics: true               # ä¿å­˜æ€§èƒ½æŒ‡æ ‡åˆ°CSV

  # é«˜çº§ä¼˜åŒ–ï¼ˆProfessionalåŠä»¥ä¸Šæ¡£ä½ï¼‰
  advanced:
    use_mixed_precision: false       # ä½¿ç”¨FP16æ··åˆç²¾åº¦ï¼ˆéœ€éªŒè¯ç²¾åº¦å½±å“ï¼‰
    async_data_transfer: false       # å¼‚æ­¥æ•°æ®ä¼ è¾“ï¼ˆCUDA streamsï¼‰
    prefetch_next_patient: false     # é¢„åŠ è½½ä¸‹ä¸€ä¸ªpatient
    dataloader_prefetch_factor: 2    # DataLoaderé¢„å–å› å­
    persistent_workers: true         # å¤ç”¨DataLoader workerè¿›ç¨‹

# ============================================================
# Original Performance Configuration (ä¿ç•™å‘åå…¼å®¹)
# ============================================================
# æ³¨æ„: å¦‚æœ hardware_detection.enabled=trueï¼Œä»¥ä¸‹é…ç½®ä¼šè¢«è‡ªåŠ¨æ£€æµ‹å€¼è¦†ç›–
# ============================================================
  # GPU memory management
  gpu_memory_fraction: 0.9

  # Clear GPU cache every N patients
  clear_cache_interval: 1            # å°†è¢«è‡ªåŠ¨æ£€æµ‹å€¼è¦†ç›–

  # Number of workers for data loading
  num_workers: 0                     # å°†è¢«è‡ªåŠ¨æ£€æµ‹å€¼è¦†ç›–

  # Pin memory for faster GPU transfer
  pin_memory: true                   # å°†è¢«è‡ªåŠ¨æ£€æµ‹å€¼è¦†ç›–
```

---

## ğŸš€ å®ç°è®¡åˆ’

### Phase 1: æ ¸å¿ƒåŸºç¡€è®¾æ–½ï¼ˆç¬¬1-2å‘¨ï¼‰

**ç›®æ ‡**: å»ºç«‹ç¡¬ä»¶æ£€æµ‹å’Œé…ç½®æ¡£ä½ç³»ç»Ÿ

**ä»»åŠ¡æ¸…å•**:

- [ ] **Task 1.1**: åˆ›å»º `core/hardware_profiler.py`
  - [ ] å®ç° `GPUProfiler` ç±»
  - [ ] å®ç° `CPUProfiler` ç±»
  - [ ] å®ç° `RAMProfiler` ç±»
  - [ ] å®ç° `DiskProfiler` ç±»
  - [ ] å•å…ƒæµ‹è¯•ï¼ˆåœ¨å¤šç§ç¡¬ä»¶ä¸Šæµ‹è¯•ï¼‰

- [ ] **Task 1.2**: åˆ›å»º `core/performance_profiles.py`
  - [ ] å®šä¹‰5ä¸ªé…ç½®æ¡£ä½ï¼ˆMinimal â†’ Enterpriseï¼‰
  - [ ] å®ç°è¯„åˆ†ç®—æ³• `select_optimal_profile()`
  - [ ] åˆ›å»ºé…ç½®æ¡£ä½æ•°æ®ç±» `PerformanceProfile`
  - [ ] å•å…ƒæµ‹è¯•

- [ ] **Task 1.3**: æ›´æ–° `core/config_manager.py`
  - [ ] æ·»åŠ ç¡¬ä»¶æ£€æµ‹é…ç½®åŠ è½½
  - [ ] æ·»åŠ å®‰å…¨é…ç½®åŠ è½½
  - [ ] é›†æˆ `PerformanceProfile` åˆ°é…ç½®ç³»ç»Ÿ
  - [ ] æ›´æ–°éªŒè¯é€»è¾‘

- [ ] **Task 1.4**: æ›´æ–°é…ç½®æ–‡ä»¶æ¨¡æ¿
  - [ ] æ›´æ–° `config/config.yaml.template`
  - [ ] æ·»åŠ è¯¦ç»†æ³¨é‡Šå’Œç¤ºä¾‹
  - [ ] åˆ›å»ºè¿ç§»æŒ‡å—ï¼ˆä»æ—§é…ç½®åˆ°æ–°é…ç½®ï¼‰

**éªŒæ”¶æ ‡å‡†**:
- ç¡¬ä»¶æ£€æµ‹åœ¨Windowså’ŒLinuxä¸Šæ­£å¸¸å·¥ä½œ
- æ‰€æœ‰5ä¸ªæ¡£ä½é…ç½®æ­£ç¡®
- å•å…ƒæµ‹è¯•è¦†ç›–ç‡ > 80%

---

### Phase 2: è¿è¡Œæ—¶ä¼˜åŒ–å¼•æ“ï¼ˆç¬¬3-4å‘¨ï¼‰

**ç›®æ ‡**: åº”ç”¨ä¼˜åŒ–é…ç½®åˆ°å®é™…æ¨ç†æµç¨‹

**ä»»åŠ¡æ¸…å•**:

- [ ] **Task 2.1**: ä¿®æ”¹ `core/ai_cac_inference_lib.py`
  - [ ] åŠ¨æ€é…ç½®DataLoaderï¼ˆnum_workers, pin_memoryç­‰ï¼‰
  - [ ] å®ç°åŠ¨æ€GPUç¼“å­˜æ¸…ç†ç­–ç•¥
  - [ ] æ·»åŠ å¼‚æ­¥æ•°æ®ä¼ è¾“ï¼ˆCUDA streamsï¼‰
  - [ ] æ·»åŠ æ··åˆç²¾åº¦æ¨ç†ï¼ˆFP16ï¼‰
  - [ ] æ€§èƒ½æµ‹è¯•

- [ ] **Task 2.2**: ä¿®æ”¹ `cli/run_nb10.py`
  - [ ] é›†æˆç¡¬ä»¶æ£€æµ‹åˆ°å¯åŠ¨æµç¨‹
  - [ ] æ·»åŠ äº¤äº’å¼æ¨¡å¼é€‰æ‹©ç•Œé¢
  - [ ] æ›´æ–°æ—¥å¿—è¾“å‡ºï¼ˆåŒ…å«æ€§èƒ½ä¿¡æ¯ï¼‰

- [ ] **Task 2.3**: æ€§èƒ½éªŒè¯
  - [ ] åœ¨RTX 2060ç¯å¢ƒæµ‹è¯•ï¼ˆStandardæ¡£ä½ï¼‰
  - [ ] åœ¨RTX 3060/3070ç¯å¢ƒæµ‹è¯•ï¼ˆPerformanceæ¡£ä½ï¼‰
  - [ ] åœ¨CPUç¯å¢ƒæµ‹è¯•ï¼ˆMinimalæ¡£ä½ï¼‰
  - [ ] æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š

**éªŒæ”¶æ ‡å‡†**:
- Standardæ¡£ä½æ€§èƒ½æå‡ 18-25%
- Performanceæ¡£ä½æ€§èƒ½æå‡ 40-50%
- æ— åŠŸèƒ½é€€åŒ–ï¼ˆAgatston scoreç»“æœä¸€è‡´ï¼‰

---

### Phase 3: å®‰å…¨ä¸ç›‘æ§ç³»ç»Ÿï¼ˆç¬¬5-6å‘¨ï¼‰

**ç›®æ ‡**: å®ç°ä¿æŠ¤æœºåˆ¶ï¼Œç¡®ä¿åŒ»ç–—çº§å¯é æ€§

**ä»»åŠ¡æ¸…å•**:

- [ ] **Task 3.1**: åˆ›å»º `core/safety_monitor.py`
  - [ ] å®ç° `OOMProtector` ç±»
  - [ ] å®ç° `TemperatureMonitor` ç±»
  - [ ] å®ç° `PerformanceTracker` ç±»
  - [ ] å®ç° `AutoDowngradeManager` ç±»
  - [ ] å•å…ƒæµ‹è¯•

- [ ] **Task 3.2**: é›†æˆåˆ°æ¨ç†æµç¨‹
  - [ ] åœ¨æ¨ç†å‰è¿›è¡Œå®‰å…¨æ£€æŸ¥
  - [ ] åœ¨æ¨ç†ä¸­ç›‘æ§æ¸©åº¦å’Œæ€§èƒ½
  - [ ] åœ¨å¼‚å¸¸æ—¶è§¦å‘ä¿æŠ¤æœºåˆ¶
  - [ ] è®°å½•æ‰€æœ‰å®‰å…¨äº‹ä»¶åˆ°æ—¥å¿—

- [ ] **Task 3.3**: å‹åŠ›æµ‹è¯•
  - [ ] OOMåœºæ™¯æµ‹è¯•ï¼ˆæ¨¡æ‹Ÿä½æ˜¾å­˜ç¯å¢ƒï¼‰
  - [ ] é«˜æ¸©åœºæ™¯æµ‹è¯•ï¼ˆæŒç»­æ»¡è½½ï¼‰
  - [ ] å¤šæ¬¡å¤±è´¥è‡ªåŠ¨é™çº§æµ‹è¯•
  - [ ] é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§æµ‹è¯•ï¼ˆ197ä¾‹è¿ç»­è¿è¡Œï¼‰

**éªŒæ”¶æ ‡å‡†**:
- OOMæ—¶è‡ªåŠ¨é™çº§ï¼Œæ— å´©æºƒ
- GPUæ¸©åº¦è¶…é˜ˆå€¼æ—¶æ­£ç¡®å¤„ç†
- å¼‚å¸¸é™çº§æœºåˆ¶æ­£å¸¸å·¥ä½œ
- ç¨³å®šæ€§æµ‹è¯•é€šè¿‡ï¼ˆ197ä¾‹æ— ä¸­æ–­ï¼‰

---

### Phase 4: ç”¨æˆ·ä½“éªŒä¼˜åŒ–ï¼ˆç¬¬7å‘¨ï¼‰

**ç›®æ ‡**: å®Œå–„äº¤äº’ç•Œé¢å’Œæ–‡æ¡£

**ä»»åŠ¡æ¸…å•**:

- [ ] **Task 4.1**: ä¼˜åŒ–å¯åŠ¨ç•Œé¢
  - [ ] ç¾åŒ–ç¡¬ä»¶æ£€æµ‹è¾“å‡º
  - [ ] æ·»åŠ è¿›åº¦æ¡å’Œå®æ—¶æ€§èƒ½æ˜¾ç¤º
  - [ ] å®Œæˆåæ˜¾ç¤ºæ€§èƒ½æ€»ç»“
  - [ ] ç”¨æˆ·ä½“éªŒæµ‹è¯•

- [ ] **Task 4.2**: æ›´æ–°æ–‡æ¡£
  - [ ] æ›´æ–° `docs/USER_MANUAL.md`ï¼ˆæ·»åŠ ç¡¬ä»¶è‡ªé€‚åº”ç« èŠ‚ï¼‰
  - [ ] æ›´æ–° `docs/INSTALLATION_GUIDE.md`ï¼ˆç¡¬ä»¶æ¨èé…ç½®ï¼‰
  - [ ] åˆ›å»º `docs/PERFORMANCE_TUNING_GUIDE.md`
  - [ ] æ›´æ–° `README.md`

- [ ] **Task 4.3**: åˆ›å»ºç¤ºä¾‹å’Œæ•™ç¨‹
  - [ ] è§†é¢‘æ•™ç¨‹ï¼šé¦–æ¬¡è¿è¡Œå’Œæ¨¡å¼é€‰æ‹©
  - [ ] æ•…éšœæ’é™¤æŒ‡å—
  - [ ] FAQæ–‡æ¡£

**éªŒæ”¶æ ‡å‡†**:
- åŒ»ç”Ÿç”¨æˆ·æµ‹è¯•åé¦ˆè‰¯å¥½ï¼ˆæ˜“ç”¨æ€§è¯„åˆ† > 4/5ï¼‰
- æ–‡æ¡£å®Œæ•´ï¼Œè¦†ç›–æ‰€æœ‰åŠŸèƒ½
- FAQè¦†ç›–å¸¸è§é—®é¢˜

---

### Phase 5: æµ‹è¯•ä¸å‘å¸ƒï¼ˆç¬¬8å‘¨ï¼‰

**ç›®æ ‡**: å…¨é¢æµ‹è¯•å¹¶å‡†å¤‡å‘å¸ƒ

**ä»»åŠ¡æ¸…å•**:

- [ ] **Task 5.1**: é›†æˆæµ‹è¯•
  - [ ] å®Œæ•´å·¥ä½œæµæµ‹è¯•ï¼ˆæ£€æµ‹â†’æ¨ç†â†’ç»“æœï¼‰
  - [ ] å¤šç¡¬ä»¶ç¯å¢ƒæµ‹è¯•çŸ©é˜µ
  - [ ] è¾¹ç•Œæƒ…å†µæµ‹è¯•
  - [ ] å›å½’æµ‹è¯•ï¼ˆç¡®ä¿ä¸v1.0å…¼å®¹ï¼‰

- [ ] **Task 5.2**: æ€§èƒ½åŸºå‡†æµ‹è¯•
  - [ ] å»ºç«‹æ€§èƒ½åŸºå‡†æ•°æ®åº“
  - [ ] ç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š
  - [ ] éªŒè¯æ‰€æœ‰æ¡£ä½æ€§èƒ½ç¬¦åˆé¢„æœŸ

- [ ] **Task 5.3**: ä»£ç å®¡æŸ¥ä¸ä¼˜åŒ–
  - [ ] Code review
  - [ ] æ€§èƒ½profiling
  - [ ] ä»£ç æ¸…ç†å’Œæ–‡æ¡£å®Œå–„

- [ ] **Task 5.4**: å‘å¸ƒå‡†å¤‡
  - [ ] æ›´æ–° `CHANGELOG.md`
  - [ ] åˆ›å»ºå‘å¸ƒè¯´æ˜
  - [ ] æ‰“åŒ…å’Œåˆ†å‘å‡†å¤‡

**éªŒæ”¶æ ‡å‡†**:
- æ‰€æœ‰æµ‹è¯•é€šè¿‡
- æ€§èƒ½æå‡è¾¾æ ‡
- æ–‡æ¡£å®Œæ•´
- å‡†å¤‡å¥½å‘å¸ƒ v2.0.0

---

## ğŸ“Š æ€§èƒ½é¢„æœŸæ€»ç»“

### å½“å‰åŸºçº¿ vs ä¼˜åŒ–åæ€§èƒ½

| ç¡¬ä»¶é…ç½® | å½“å‰æ€§èƒ½ | ä¼˜åŒ–åæ¡£ä½ | é¢„æœŸæ€§èƒ½ | æå‡å¹…åº¦ | 197ä¾‹æ€»è€—æ—¶ |
|---------|---------|----------|---------|---------|------------|
| RTX 2060 6GB + 16GB RAM | 35-40ç§’/ä¾‹ | Standard | 28-32ç§’/ä¾‹ | **â†“ 20-25%** | 1.6-1.8å°æ—¶ (vs 2.0å°æ—¶) |
| RTX 3060 12GB + 32GB RAM | 30-35ç§’/ä¾‹ | Performance | 18-22ç§’/ä¾‹ | **â†“ 40-45%** | 1.0-1.2å°æ—¶ |
| RTX 4080 16GB + 64GB RAM | 25-30ç§’/ä¾‹ | Professional | 12-15ç§’/ä¾‹ | **â†“ 50-55%** | 0.65-0.8å°æ—¶ |
| CPU only (i7 8æ ¸) | 120-180ç§’/ä¾‹ | Minimal | 100-150ç§’/ä¾‹ | â†“ 15-20% | 5.5-8.2å°æ—¶ |

---

## âš ï¸ é£é™©ä¸é™åˆ¶

### æŠ€æœ¯é£é™©

1. **æ··åˆç²¾åº¦ç²¾åº¦æŸå¤±**
   - **é£é™©**: FP16å¯èƒ½å½±å“Agatston scoreè®¡ç®—ç²¾åº¦
   - **ç¼“è§£**: éœ€è¦éªŒè¯æ··åˆç²¾åº¦ç»“æœä¸FP32çš„ä¸€è‡´æ€§
   - **å»ºè®®**: åœ¨ProfessionalåŠä»¥ä¸Šæ¡£ä½å¯é€‰å¯ç”¨ï¼Œé»˜è®¤å…³é—­

2. **Windowså¤šè¿›ç¨‹å…¼å®¹æ€§**
   - **é£é™©**: Windowsä¸ŠDataLoaderå¤šè¿›ç¨‹å¯èƒ½ä¸ç¨³å®š
   - **ç¼“è§£**: ä¿å®ˆè®¾ç½®num_workersï¼ˆæœ€å¤š4-6ï¼‰ï¼Œæä¾›å›é€€æœºåˆ¶
   - **å»ºè®®**: å……åˆ†æµ‹è¯•Windowsç¯å¢ƒ

3. **GPUæ¸©åº¦ç›‘æ§ä¾èµ–**
   - **é£é™©**: `pynvml`æˆ–`nvidia-smi`å¯èƒ½ä¸å¯ç”¨
   - **ç¼“è§£**: æ¸©åº¦ç›‘æ§ä½œä¸ºå¯é€‰åŠŸèƒ½ï¼Œå¤±è´¥æ—¶ä¸å½±å“ä¸»æµç¨‹
   - **å»ºè®®**: æä¾›æ˜ç¡®çš„ä¾èµ–å®‰è£…æŒ‡å—

### ç¡¬ä»¶é™åˆ¶

1. **6GB GPUæ˜¾å­˜çº¦æŸ**
   - **é™åˆ¶**: æ— æ³•å¤§å¹…æå‡SLICE_BATCH_SIZE
   - **åº”å¯¹**: Standardæ¡£ä½ä¿å®ˆé…ç½®ï¼Œç¡®ä¿ç¨³å®šæ€§
   - **å»ºè®®**: åœ¨æ–‡æ¡£ä¸­è¯´æ˜å‡çº§åˆ°12GB GPUçš„æ€§èƒ½æå‡

2. **Windows forkæ€§èƒ½**
   - **é™åˆ¶**: Windowsä¸Šå¤šè¿›ç¨‹æ€§èƒ½ä¸å¦‚Linux
   - **åº”å¯¹**: é€‚å½“é™ä½num_workersé¢„æœŸ
   - **å»ºè®®**: åœ¨Linuxä¸Šå¯ä»¥æ›´æ¿€è¿›çš„é…ç½®

### åŒ»ç–—åº”ç”¨é™åˆ¶

1. **ç»“æœä¸€è‡´æ€§è¦æ±‚**
   - **è¦æ±‚**: ä¼˜åŒ–åçš„Agatston scoreå¿…é¡»ä¸ä¼˜åŒ–å‰ä¸€è‡´
   - **éªŒè¯**: éœ€è¦åœ¨å¤šç§ç¡¬ä»¶ä¸ŠéªŒè¯ç»“æœä¸€è‡´æ€§
   - **æµ‹è¯•**: Phase 4éœ€è¦åŒ…å«å®Œæ•´çš„ä¸€è‡´æ€§æµ‹è¯•

2. **ç¨³å®šæ€§ä¼˜å…ˆ**
   - **åŸåˆ™**: åŒ»ç–—åœºæ™¯ä¸‹ç¨³å®šæ€§ > æ€§èƒ½
   - **è®¾è®¡**: æä¾›"Stable"æ¨¡å¼ä½œä¸ºå®‰å…¨é€‰é¡¹
   - **å»ºè®®**: é»˜è®¤æ¡£ä½åä¿å®ˆï¼Œç”¨æˆ·å¯æ‰‹åŠ¨é€‰æ‹©æ¿€è¿›æ¨¡å¼

---

## ğŸ“– å…³é”®é—®é¢˜å›ç­”

### 1. ç½‘ç»œè¿æ¥éœ€æ±‚

**å›ç­”**: **ä¸éœ€è¦ç½‘ç»œè¿æ¥**

- æ‰€æœ‰ç¡¬ä»¶æ£€æµ‹å®Œå…¨æœ¬åœ°è¿›è¡Œ
- æ— éœ€è®¿é—®å¤–éƒ¨APIæˆ–æœåŠ¡
- é…ç½®æ–‡ä»¶å’Œæ¨¡å‹å‡ä¸ºæœ¬åœ°æ–‡ä»¶
- **å¯é€‰åŠŸèƒ½**: ä¸Šä¼ åŒ¿åç¡¬ä»¶ç»Ÿè®¡ç”¨äºä¼˜åŒ–ï¼ˆéœ€ç”¨æˆ·åŒæ„ï¼Œé»˜è®¤å…³é—­ï¼‰

---

### 2. æœªçŸ¥ç¡¬ä»¶å¤„ç†ç­–ç•¥

**å›ç­”**: **å¼•å¯¼ç”¨æˆ·æ‰‹åŠ¨é€‰æ‹©ï¼Œå»ºè®®ä¿å®ˆæ¡£ä½**

**å¤„ç†æµç¨‹**:

1. **æ£€æµ‹å¤±è´¥æ—¶**:
   ```
   âš ï¸ æ— æ³•è‡ªåŠ¨æ£€æµ‹ç¡¬ä»¶é…ç½®

   å¯èƒ½åŸå› :
   - GPUé©±åŠ¨æœªæ­£ç¡®å®‰è£…
   - ä¸æ”¯æŒçš„ç¡¬ä»¶

   è¯·æ‰‹åŠ¨é€‰æ‹©é…ç½®æ¡£ä½:
     [1] Minimal - æœ€ä½é…ç½®ï¼ˆæ¨èï¼Œç¨³å®šæ€§æœ€é«˜ï¼‰
     [2] Standard - æ ‡å‡†é…ç½®
     [3] Performance - é«˜æ€§èƒ½é…ç½®

   å»ºè®®: é¦–æ¬¡è¿è¡Œè¯·é€‰æ‹© [1] Minimal è¿›è¡Œæµ‹è¯•
   ```

2. **å¼‚å¸¸ç¡¬ä»¶ç»„åˆ**:
   - ä¾‹å¦‚: 4GB GPU + 64GB RAMï¼ˆä¸å‡è¡¡ï¼‰
   - ç³»ç»Ÿä¼šæ˜¾ç¤ºè­¦å‘Šï¼Œå»ºè®®ä½¿ç”¨ä¿å®ˆé…ç½®
   - ç”¨æˆ·å¯é€‰æ‹©å¿½ç•¥è­¦å‘Š

3. **Fallbackç­–ç•¥**:
   - é»˜è®¤å›é€€åˆ° `Minimal` æ¡£ä½
   - è®°å½•è­¦å‘Šæ—¥å¿—
   - å¼•å¯¼ç”¨æˆ·æŸ¥çœ‹æ–‡æ¡£æˆ–è”ç³»æ”¯æŒ

---

### 3. æ‰‹åŠ¨é…ç½®è¦†ç›–

**å›ç­”**: **é…ç½®å¯æ‰‹åŠ¨è¦†ç›–ï¼Œé»˜è®¤ä¸ºè‡ªåŠ¨æ¨¡å¼**

**é…ç½®ä¼˜å…ˆçº§**ï¼ˆä»é«˜åˆ°ä½ï¼‰:

```
1. å‘½ä»¤è¡Œå‚æ•° (--num-workers 4)
   â†“
2. é…ç½®æ–‡ä»¶æ‰‹åŠ¨è®¾ç½® (hardware_detection.mode: "manual")
   â†“
3. è‡ªåŠ¨æ£€æµ‹ç»“æœ (hardware_detection.mode: "auto", é»˜è®¤)
   â†“
4. ç³»ç»Ÿé»˜è®¤å€¼ (Minimalæ¡£ä½)
```

**æ‰‹åŠ¨è¦†ç›–æ–¹å¼**:

**æ–¹å¼1: ä¿®æ”¹é…ç½®æ–‡ä»¶**
```yaml
# config.yaml
performance:
  hardware_detection:
    enabled: true
    mode: "manual"               # åˆ‡æ¢åˆ°æ‰‹åŠ¨æ¨¡å¼
    profile: "performance"       # æ‰‹åŠ¨æŒ‡å®šæ¡£ä½

  # æˆ–è€…ç›´æ¥è¦†ç›–å•ä¸ªå‚æ•°
  num_workers: 4                 # è¦†ç›–è‡ªåŠ¨æ£€æµ‹å€¼
  pin_memory: true
```

**æ–¹å¼2: å‘½ä»¤è¡Œå‚æ•°**
```bash
# è¦†ç›–num_workers
python cli/run_nb10.py --config config.yaml --num-workers 4

# è¦†ç›–æ¡£ä½
python cli/run_nb10.py --config config.yaml --profile performance

# ç¦ç”¨è‡ªåŠ¨æ£€æµ‹
python cli/run_nb10.py --config config.yaml --no-auto-detect
```

**æ–¹å¼3: äº¤äº’å¼é€‰æ‹©**
```
è¯·é€‰æ‹© [1-4] (ç›´æ¥å›è½¦ä½¿ç”¨æ¨è): 4

è‡ªå®šä¹‰æ¨¡å¼:
  num_workers [æ¨è: 2]: 4
  pin_memory [æ¨è: true]: true
  slice_batch_size [æ¨è: 4]: 6
  ...
```

---

## ğŸ“Œ åç»­å·¥ä½œ

### æœªæ¥å¢å¼ºåŠŸèƒ½ï¼ˆv2.1+ï¼‰

1. **å­¦ä¹ å¼ä¼˜åŒ–**
   - è®°å½•å†å²è¿è¡Œæ•°æ®
   - åŸºäºå®é™…æ€§èƒ½è‡ªåŠ¨è°ƒæ•´é…ç½®
   - ä¸ªæ€§åŒ–ç¡¬ä»¶Profile

2. **äº‘ç«¯é…ç½®åŒæ­¥**ï¼ˆå¯é€‰ï¼‰
   - ç”¨æˆ·å¯é€‰æ‹©ä¸Šä¼ åŒ¿åç¡¬ä»¶æ•°æ®
   - è·å–ç¤¾åŒºä¼˜åŒ–é…ç½®æ¨è
   - å¸®åŠ©æ”¹è¿›è‡ªåŠ¨æ£€æµ‹ç®—æ³•

3. **å¤šGPUå¹¶è¡Œ**
   - Enterpriseæ¡£ä½æ”¯æŒå¤šGPU
   - æ‰¹é‡å¤„ç†patientsåˆ°ä¸åŒGPU
   - æ˜¾è‘—æå‡ååé‡

4. **é«˜çº§æ€§èƒ½åˆ†æå·¥å…·**
   - å†…ç½®profiler
   - ç“¶é¢ˆå¯è§†åŒ–
   - ä¼˜åŒ–å»ºè®®ç”Ÿæˆå™¨

---

## ğŸ“„ é™„å½•

### A. ä¾èµ–åŒ…åˆ—è¡¨

æ–°å¢ä¾èµ–:

```
psutil>=5.9.0           # CPU/RAMæ£€æµ‹
pynvml>=11.5.0          # GPUæ¸©åº¦ç›‘æ§ï¼ˆå¯é€‰ï¼‰
```

### B. æµ‹è¯•ç¡¬ä»¶çŸ©é˜µ

å»ºè®®åœ¨ä»¥ä¸‹ç¡¬ä»¶ä¸Šè¿›è¡Œæµ‹è¯•:

| åˆ†ç±» | GPU | RAM | CPU | OS |
|------|-----|-----|-----|-----|
| ä½ç«¯ | GTX 1650 4GB | 8GB | i3 4æ ¸ | Windows 10 |
| æ ‡å‡† | RTX 2060 6GB | 16GB | i5 6æ ¸ | Windows 11 |
| é«˜ç«¯ | RTX 3060 12GB | 32GB | i7 8æ ¸ | Windows 11 |
| ä¸“ä¸š | RTX 4080 16GB | 64GB | i9 12æ ¸ | Linux |
| CPU | æ—  | 16GB | i7 8æ ¸ | Windows 11 |

### C. æ€§èƒ½æµ‹è¯•åè®®

æ ‡å‡†æµ‹è¯•æµç¨‹:

1. ä½¿ç”¨ç›¸åŒçš„30ä¾‹æµ‹è¯•æ•°æ®é›†
2. è®°å½•æ¯ä¾‹çš„å¤„ç†æ—¶é—´å’Œç»“æœ
3. éªŒè¯Agatston scoreä¸€è‡´æ€§ï¼ˆè¯¯å·® < 0.1%ï¼‰
4. ç›‘æ§GPUæ¸©åº¦ã€æ˜¾å­˜å ç”¨ã€CPU/RAMä½¿ç”¨ç‡
5. ç”Ÿæˆæ€§èƒ½æŠ¥å‘Šå’Œå¯¹æ¯”å›¾è¡¨

### D. å‚è€ƒèµ„æ–™

- PyTorch DataLoaderä¼˜åŒ–: https://pytorch.org/docs/stable/data.html
- CUDA Best Practices: https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/
- MONAIæ€§èƒ½ä¼˜åŒ–: https://docs.monai.io/en/stable/performance.html

---

## âœ… æ–‡æ¡£çŠ¶æ€

- **ç‰ˆæœ¬**: 1.0.0
- **çŠ¶æ€**: è®¾è®¡ææ¡ˆ (Proposal)
- **å®¡æ ¸çŠ¶æ€**: å¾…å®¡æ ¸
- **å®æ–½çŠ¶æ€**: æœªå¼€å§‹

**å˜æ›´å†å²**:
- 2025-10-14: åˆå§‹ç‰ˆæœ¬ï¼ŒåŸºäºæ€§èƒ½åˆ†æè®¨è®ºåˆ›å»º

---

## ğŸ‘¥ è´¡çŒ®è€…

- **è®¾è®¡**: Claude (AI Assistant) + é™ˆåŒ»ç”Ÿå›¢é˜Ÿ
- **æ€§èƒ½åˆ†æ**: åŸºäº `tools/nb10_windows` ä»£ç å®¡æŸ¥
- **æµ‹è¯•ç¯å¢ƒ**: RTX 2060 6GB + 16GB RAM + Windows

---

## ğŸ“ è”ç³»ä¸åé¦ˆ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼åé¦ˆ:

- é¡¹ç›®Issue: (å¾…æ·»åŠ GitHubä»“åº“é“¾æ¥)
- æ–‡æ¡£Issue: åœ¨æœ¬æ–‡æ¡£æ‰€åœ¨ç›®å½•åˆ›å»º `FEEDBACK.md`

---

**æ–‡æ¡£ç»“æŸ**

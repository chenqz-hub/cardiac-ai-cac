"""
æ€§èƒ½é…ç½®æ¡£ä½ç³»ç»Ÿ
Performance Profile System

æ ¹æ®ç¡¬ä»¶é…ç½®è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„DataLoaderå’Œæ¨ç†å‚æ•°ã€‚
"""

from enum import Enum
from dataclasses import dataclass
from typing import Optional
import logging

logger = logging.getLogger(__name__)


class ProfileTier(Enum):
    """é…ç½®æ¡£ä½ç­‰çº§"""
    MINIMAL = 1      # æœ€ä½é…ç½® (CPUæˆ–<4GB VRAM)
    STANDARD = 2     # æ ‡å‡†é…ç½® (6GB VRAM, RTX 2060ç±»)
    PERFORMANCE = 3  # æ€§èƒ½é…ç½® (8-12GB VRAM, RTX 3060ç±»)
    PROFESSIONAL = 4 # ä¸“ä¸šé…ç½® (16-24GB VRAM, RTX 4080ç±»)
    ENTERPRISE = 5   # ä¼ä¸šé…ç½® (å¤šGPUæˆ–>24GB VRAM)


@dataclass
class PerformanceProfile:
    """æ€§èƒ½é…ç½®å‚æ•°"""
    # æ¡£ä½ä¿¡æ¯
    tier: ProfileTier
    tier_name: str
    description: str

    # DataLoaderä¼˜åŒ–å‚æ•°
    num_workers: int          # æ•°æ®åŠ è½½çº¿ç¨‹æ•°
    pin_memory: bool          # æ˜¯å¦ä½¿ç”¨pinned memory
    prefetch_factor: Optional[int]  # é¢„å–å› å­

    # æ¨ç†æ‰¹å¤„ç†å‚æ•°
    slice_batch_size: int     # åˆ‡ç‰‡æ‰¹å¤§å°

    # ç¼“å­˜ç®¡ç†å‚æ•°
    clear_cache_interval: int # å¤šå°‘ä¸ªpatientæ¸…ç†ä¸€æ¬¡ç¼“å­˜
    enable_auto_cache: bool   # æ˜¯å¦å¯ç”¨è‡ªåŠ¨ç¼“å­˜ç®¡ç†

    # é¢„æœŸæ€§èƒ½
    expected_speedup: str     # é¢„æœŸæ€§èƒ½æå‡
    expected_time_per_patient: float  # é¢„æœŸå¤„ç†æ—¶é—´(ç§’/æ‚£è€…)

    def __str__(self):
        return (f"æ¡£ä½: {self.tier_name}\n"
                f"  - num_workers: {self.num_workers}\n"
                f"  - pin_memory: {self.pin_memory}\n"
                f"  - slice_batch_size: {self.slice_batch_size}\n"
                f"  - é¢„æœŸæå‡: {self.expected_speedup}")


# é¢„å®šä¹‰é…ç½®æ¡£ä½
PROFILES = {
    ProfileTier.MINIMAL: PerformanceProfile(
        tier=ProfileTier.MINIMAL,
        tier_name="Minimal",
        description="CPU mode or GPU VRAM<4GB",
        num_workers=2,              # v1.1.3: CPUå¤šçº¿ç¨‹ä¼˜åŒ–ï¼ˆä»0â†’2ï¼‰
        pin_memory=False,           # CPUä¸éœ€è¦pin_memory
        prefetch_factor=2,          # v1.1.3: æ–°å¢é¢„å–ä¼˜åŒ–
        slice_batch_size=8,         # v1.1.3: CPUå¯ç”¨æ›´å¤§batchï¼ˆä»2â†’8ï¼‰
        clear_cache_interval=1,
        enable_auto_cache=True,
        expected_speedup="â†‘ 30-40% (v1.1.3ä¼˜åŒ–)",  # v1.1.3: é¢„æœŸæå‡
        expected_time_per_patient=10.5  # 15ç§’ * 0.70 â‰ˆ 10.5ç§’
    ),

    ProfileTier.STANDARD: PerformanceProfile(
        tier=ProfileTier.STANDARD,
        tier_name="Standard",
        description="6GB VRAM (RTX 2060/3050/4050)",
        num_workers=2,              # â† é»˜è®¤å¯ç”¨ï¼ˆRAMä¸è¶³æ—¶è‡ªåŠ¨é™ä¸º0ï¼‰
        pin_memory=True,            # â† é»˜è®¤å¯ç”¨ï¼ˆRAMä¸è¶³æ—¶è‡ªåŠ¨ç¦ç”¨ï¼‰
        prefetch_factor=2,
        slice_batch_size=4,
        clear_cache_interval=1,
        enable_auto_cache=True,
        expected_speedup="â†‘ 20-30%",
        expected_time_per_patient=11.0  # 15ç§’ * 0.73 â‰ˆ 11ç§’
    ),

    ProfileTier.PERFORMANCE: PerformanceProfile(
        tier=ProfileTier.PERFORMANCE,
        tier_name="Performance",
        description="8-12GB VRAM (RTX 3060/4060)",
        num_workers=4,
        pin_memory=True,
        prefetch_factor=2,
        slice_batch_size=6,
        clear_cache_interval=3,
        enable_auto_cache=True,
        expected_speedup="â†‘ 35-45%",
        expected_time_per_patient=9.0   # 15ç§’ * 0.60 â‰ˆ 9ç§’
    ),

    ProfileTier.PROFESSIONAL: PerformanceProfile(
        tier=ProfileTier.PROFESSIONAL,
        tier_name="Professional",
        description="16-24GB VRAM (RTX 4080/A5000)",
        num_workers=6,
        pin_memory=True,
        prefetch_factor=3,
        slice_batch_size=8,
        clear_cache_interval=5,
        enable_auto_cache=True,
        expected_speedup="â†‘ 50-60%",
        expected_time_per_patient=7.0   # 15ç§’ * 0.47 â‰ˆ 7ç§’
    ),

    ProfileTier.ENTERPRISE: PerformanceProfile(
        tier=ProfileTier.ENTERPRISE,
        tier_name="Enterprise",
        description="Multi-GPU or >24GB VRAM (A6000/H100)",
        num_workers=8,
        pin_memory=True,
        prefetch_factor=4,
        slice_batch_size=12,
        clear_cache_interval=10,
        enable_auto_cache=True,
        expected_speedup="â†‘ 70-80%+",
        expected_time_per_patient=5.0   # 15ç§’ * 0.33 â‰ˆ 5ç§’
    ),
}


def select_profile_by_hardware(hw_info) -> PerformanceProfile:
    """
    æ ¹æ®ç¡¬ä»¶ä¿¡æ¯è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜é…ç½®æ¡£ä½

    Args:
        hw_info: HardwareInfoå¯¹è±¡ (æ¥è‡ªhardware_profiler)

    Returns:
        PerformanceProfile: æœ€ä¼˜é…ç½®æ¡£ä½

    æ¡£ä½é€‰æ‹©è§„åˆ™:
    - MINIMAL: CPUæ¨¡å¼ æˆ– GPU VRAM < 4GB
    - STANDARD: 4-7GB VRAM (RTX 2060, 3050)
    - PERFORMANCE: 8-12GB VRAM (RTX 3060, 4060)
    - PROFESSIONAL: 13-24GB VRAM (RTX 4080, A5000)
    - ENTERPRISE: å¤šGPU æˆ– >24GB VRAM
    """
    gpu = hw_info.gpu
    cpu = hw_info.cpu
    ram = hw_info.ram

    # è§„åˆ™1: æ— GPUæˆ–VRAMä¸è¶³ â†’ MINIMAL
    if not gpu.available or gpu.vram_total_gb < 4.0:
        logger.info("é€‰æ‹©æ¡£ä½: MINIMAL (CPUæ¨¡å¼æˆ–VRAMä¸è¶³)")
        return PROFILES[ProfileTier.MINIMAL]

    # è§„åˆ™2: å¤šGPU â†’ ENTERPRISE
    if gpu.device_count > 1:
        logger.info(f"é€‰æ‹©æ¡£ä½: ENTERPRISE (æ£€æµ‹åˆ°{gpu.device_count}ä¸ªGPU)")
        return PROFILES[ProfileTier.ENTERPRISE]

    # è§„åˆ™3: æ ¹æ®VRAMå¤§å°é€‰æ‹©
    vram = gpu.vram_total_gb

    if vram >= 24:
        tier = ProfileTier.ENTERPRISE
    elif vram >= 13:
        tier = ProfileTier.PROFESSIONAL
    elif vram >= 8:
        tier = ProfileTier.PERFORMANCE
    elif vram >= 4:
        tier = ProfileTier.STANDARD
    else:
        tier = ProfileTier.MINIMAL

    profile = PROFILES[tier]

    # å†…å­˜æ£€æŸ¥: å¦‚æœå¯ç”¨RAMä¸è¶³ï¼Œé™çº§num_workerså’Œpin_memory
    # åŸå› : æµ‹è¯•å‘ç°pin_memoryåœ¨ä½RAMç¯å¢ƒ(<8GB)ä¸‹ä¼šäº§ç”Ÿè´Ÿé¢å½±å“ (-3.2%)
    # è¯¦è§: docs/PHASE1_PERFORMANCE_TEST_REPORT.md
    if not ram.is_sufficient:  # <8GBå¯ç”¨
        logger.warning(f"å¯ç”¨å†…å­˜ä¸è¶³({ram.available_gb:.1f}GB)ï¼Œç¦ç”¨pin_memoryå’Œé™ä½num_workers")
        # åˆ›å»ºä¿®æ”¹åçš„profileå‰¯æœ¬
        profile = PerformanceProfile(
            tier=profile.tier,
            tier_name=profile.tier_name,
            description=profile.description,
            num_workers=max(0, profile.num_workers - 2),  # é™çº§
            pin_memory=False,  # â† ç¦ç”¨pin_memoryï¼ˆä½RAMä¸‹ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼‰
            prefetch_factor=profile.prefetch_factor,
            slice_batch_size=profile.slice_batch_size,
            clear_cache_interval=profile.clear_cache_interval,
            enable_auto_cache=profile.enable_auto_cache,
            expected_speedup="åŸºçº¿ (å†…å­˜ä¸è¶³ï¼Œå·²ç¦ç”¨ä¼˜åŒ–)",
            expected_time_per_patient=15.0  # å›é€€åˆ°åŸºçº¿æ€§èƒ½
        )

    logger.info(f"é€‰æ‹©æ¡£ä½: {profile.tier_name}")
    logger.info(f"  VRAM: {vram:.1f}GB, CPU: {cpu.physical_cores}æ ¸, RAM: {ram.total_gb:.1f}GB")
    logger.info(f"  é¢„æœŸæ€§èƒ½æå‡: {profile.expected_speedup}")

    return profile


def print_profile_summary(profile: PerformanceProfile):
    """
    æ‰“å°é…ç½®æ¡£ä½æ‘˜è¦

    Args:
        profile: PerformanceProfileå¯¹è±¡
    """
    print("\n" + "="*70)
    print(f"âš™ï¸  æ€§èƒ½é…ç½®æ¡£ä½: {profile.tier_name}")
    print("="*70)
    print(f"è¯´æ˜: {profile.description}")
    print()
    print("DataLoaderä¼˜åŒ–:")
    print(f"  - num_workers: {profile.num_workers} (æ•°æ®åŠ è½½çº¿ç¨‹)")
    print(f"  - pin_memory: {profile.pin_memory} (GPUå†…å­˜é”å®š)")
    if profile.prefetch_factor:
        print(f"  - prefetch_factor: {profile.prefetch_factor} (é¢„å–æ‰¹æ¬¡)")
    print()
    print("æ¨ç†å‚æ•°:")
    print(f"  - slice_batch_size: {profile.slice_batch_size}")
    print(f"  - clear_cache_interval: {profile.clear_cache_interval}")
    print()
    print("é¢„æœŸæ€§èƒ½:")
    print(f"  - æ€§èƒ½æå‡: {profile.expected_speedup}")
    print(f"  - å¤„ç†æ—¶é—´: ~{profile.expected_time_per_patient:.0f}ç§’/æ‚£è€…")
    print("="*70 + "\n")


def get_profile_comparison_table():
    """è¿”å›æ‰€æœ‰æ¡£ä½çš„å¯¹æ¯”è¡¨æ ¼ï¼ˆç”¨äºæ–‡æ¡£ï¼‰"""
    header = f"{'æ¡£ä½':<12} {'VRAM':<8} {'Workers':<8} {'Pin':<5} {'Batch':<6} {'é¢„æœŸæå‡':<12} {'æ—¶é—´':<10}"
    separator = "="*75

    rows = [separator, header, separator]

    for tier in ProfileTier:
        p = PROFILES[tier]
        row = (f"{p.tier_name:<12} "
               f"{p.description[:7]:<8} "
               f"{p.num_workers:<8} "
               f"{'æ˜¯' if p.pin_memory else 'å¦':<5} "
               f"{p.slice_batch_size:<6} "
               f"{p.expected_speedup:<12} "
               f"{p.expected_time_per_patient:.0f}ç§’")
        rows.append(row)

    rows.append(separator)
    return "\n".join(rows)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("\næµ‹è¯•æ€§èƒ½é…ç½®æ¡£ä½ç³»ç»Ÿ...")
    print("="*70)

    # æ˜¾ç¤ºæ‰€æœ‰æ¡£ä½å¯¹æ¯”
    print("\nğŸ“Š æ€§èƒ½æ¡£ä½å¯¹æ¯”è¡¨:")
    print(get_profile_comparison_table())

    # æ¨¡æ‹Ÿç¡¬ä»¶æ£€æµ‹å¹¶é€‰æ‹©æ¡£ä½
    print("\n\nğŸ” æ¨¡æ‹Ÿç¡¬ä»¶æ£€æµ‹å’Œæ¡£ä½é€‰æ‹©:")
    print("-"*70)

    from hardware_profiler import detect_hardware

    try:
        hw = detect_hardware()
        profile = select_profile_by_hardware(hw)
        print_profile_summary(profile)

        print("âœ… éªŒæ”¶æ ‡å‡†æ£€æŸ¥:")
        print(f"  - æ¡£ä½é€‰æ‹©: {profile.tier_name}")
        print(f"  - num_workers: {profile.num_workers}")
        print(f"  - pin_memory: {profile.pin_memory}")
        print(f"  - é¢„æœŸæå‡: {profile.expected_speedup}")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

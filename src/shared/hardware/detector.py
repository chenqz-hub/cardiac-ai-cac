"""
ç¡¬ä»¶æ£€æµ‹æ¨¡å— - å…±äº«ç‰ˆæœ¬
Hardware Detection Module - Shared Version

æ”¯æŒå¤šç¯å¢ƒï¼šWindows/Linux/Colab
ç”¨äºè‡ªåŠ¨æ£€æµ‹GPUã€CPUã€å†…å­˜ç­‰ç¡¬ä»¶ä¿¡æ¯ï¼Œä¸ºæ€§èƒ½ä¼˜åŒ–æä¾›åŸºç¡€æ•°æ®ã€‚

æå‡è‡ª: tools/nb10_windows/core/hardware_profiler.py
æ‰©å±•: å¢åŠ CPUç¼“å­˜æ£€æµ‹ã€Colabç¯å¢ƒæ”¯æŒã€æ›´è¯¦ç»†çš„CPUä¿¡æ¯
"""

import torch
import psutil
import platform
import logging
import os
from dataclasses import dataclass
from typing import Optional, Dict, Any
from pathlib import Path

logger = logging.getLogger(__name__)


@dataclass
class GPUInfo:
    """GPUä¿¡æ¯"""
    available: bool
    device_name: str
    vram_total_gb: float
    vram_available_gb: float
    cuda_version: Optional[str] = None
    device_count: int = 0
    compute_capability: Optional[tuple] = None  # æ–°å¢ï¼šè®¡ç®—èƒ½åŠ›

    def __post_init__(self):
        if self.available and self.cuda_version is None:
            try:
                self.cuda_version = torch.version.cuda
            except:
                self.cuda_version = "Unknown"


@dataclass
class CPUInfo:
    """CPUä¿¡æ¯ï¼ˆå¢å¼ºç‰ˆï¼ŒåŒ»é™¢CPUä¼˜åŒ–å…³é”®ï¼‰"""
    physical_cores: int
    logical_cores: int
    cpu_model: str
    cpu_freq_mhz: Optional[float] = None
    cpu_freq_min_mhz: Optional[float] = None  # æ–°å¢ï¼šæœ€ä½é¢‘ç‡
    cpu_freq_max_mhz: Optional[float] = None  # æ–°å¢ï¼šæœ€é«˜é¢‘ç‡
    cache_size_mb: Optional[float] = None     # æ–°å¢ï¼šç¼“å­˜å¤§å°ï¼ˆé‡è¦ï¼‰
    architecture: Optional[str] = None         # æ–°å¢ï¼šæ¶æ„ï¼ˆx86_64/aarch64ï¼‰

    def __post_init__(self):
        # æ£€æµ‹CPUé¢‘ç‡
        if self.cpu_freq_mhz is None:
            try:
                freq = psutil.cpu_freq()
                if freq:
                    self.cpu_freq_mhz = freq.current
                    self.cpu_freq_min_mhz = freq.min
                    self.cpu_freq_max_mhz = freq.max
            except:
                pass

        # æ£€æµ‹æ¶æ„
        if self.architecture is None:
            self.architecture = platform.machine()

    @property
    def is_high_performance(self) -> bool:
        """æ˜¯å¦é«˜æ€§èƒ½CPUï¼ˆ8æ ¸+ï¼‰"""
        return self.physical_cores >= 8

    @property
    def recommended_workers(self) -> int:
        """æ¨èçš„DataLoader workeræ•°"""
        # åŒ»é™¢CPUä¼˜åŒ–å…³é”®å‚æ•°
        if self.physical_cores <= 2:
            return 0  # Minimalæ¡£ä½ï¼šå•çº¿ç¨‹
        elif self.physical_cores <= 4:
            return 2  # Standardæ¡£ä½ä½ç«¯
        elif self.physical_cores <= 8:
            return 4  # Standardæ¡£ä½æ ‡å‡†
        elif self.physical_cores <= 16:
            return 8  # Performanceæ¡£ä½
        else:
            return 16  # Professionalæ¡£ä½


@dataclass
class RAMInfo:
    """å†…å­˜ä¿¡æ¯"""
    total_gb: float
    available_gb: float
    percent_used: float

    @property
    def is_sufficient(self) -> bool:
        """æ˜¯å¦æœ‰è¶³å¤Ÿå†…å­˜ï¼ˆå»ºè®®è‡³å°‘6GBå¯ç”¨ï¼Œç†æƒ³8GB+ï¼‰"""
        # åŸºäºNB10å®æµ‹: 3GBå³å¯è·å¾—17.2%æå‡ï¼Œè®¾ç½®6GBä¸ºå®‰å…¨é˜ˆå€¼
        return self.available_gb >= 6.0

    @property
    def recommended_batch_size(self) -> int:
        """æ¨èçš„batch size"""
        # åŸºäºå¯ç”¨å†…å­˜æ¨èbatch sizeï¼ˆåŒ»é™¢CPUä¼˜åŒ–å…³é”®ï¼‰
        if self.available_gb < 4:
            return 1
        elif self.available_gb < 8:
            return 2
        elif self.available_gb < 16:
            return 4
        else:
            return 8


@dataclass
class EnvironmentInfo:
    """ç¯å¢ƒä¿¡æ¯ï¼ˆæ–°å¢æ¨¡å—ï¼‰"""
    runtime_type: str  # 'windows' / 'linux' / 'colab' / 'wsl'
    is_colab: bool
    is_wsl: bool
    os_name: str
    os_version: str

    @property
    def is_hospital_environment(self) -> bool:
        """æ˜¯å¦å¯èƒ½æ˜¯åŒ»é™¢ç¯å¢ƒï¼ˆWindowsä¸”éColabï¼‰"""
        return self.runtime_type == 'windows' and not self.is_colab


@dataclass
class HardwareInfo:
    """å®Œæ•´ç¡¬ä»¶ä¿¡æ¯"""
    gpu: GPUInfo
    cpu: CPUInfo
    ram: RAMInfo
    environment: EnvironmentInfo  # æ–°å¢
    platform: str
    python_version: str

    @property
    def recommended_device(self) -> str:
        """æ¨èçš„è®¡ç®—è®¾å¤‡"""
        return "cuda" if self.gpu.available else "cpu"

    @property
    def performance_tier(self) -> str:
        """
        æ€§èƒ½æ¡£ä½ï¼ˆ5æ¡£ï¼‰
        Minimal â†’ Standard â†’ Performance â†’ Professional â†’ Enterprise
        """
        if self.gpu.available:
            if self.gpu.vram_total_gb >= 16:
                return "Enterprise"
            elif self.gpu.vram_total_gb >= 6:
                return "Professional"
            else:
                return "Performance"
        else:
            # çº¯CPUæ¡£ä½ï¼ˆåŒ»é™¢ç¯å¢ƒå…³é”®ï¼‰
            if self.cpu.physical_cores >= 16 and self.ram.total_gb >= 32:
                return "Performance"
            elif self.cpu.physical_cores >= 8 and self.ram.total_gb >= 16:
                return "Standard"
            else:
                return "Minimal"


def detect_environment() -> EnvironmentInfo:
    """
    æ£€æµ‹è¿è¡Œç¯å¢ƒï¼ˆæ–°å¢å‡½æ•°ï¼‰

    æ”¯æŒæ£€æµ‹:
    - Google Colab
    - WSL (Windows Subsystem for Linux)
    - åŸç”ŸWindows
    - åŸç”ŸLinux
    """
    os_name = platform.system()
    os_version = platform.release()

    # æ£€æµ‹Colab
    is_colab = False
    try:
        import google.colab
        is_colab = True
    except:
        pass

    # æ£€æµ‹WSL
    is_wsl = False
    if os_name == "Linux":
        try:
            with open('/proc/version', 'r') as f:
                version_info = f.read().lower()
                is_wsl = 'microsoft' in version_info or 'wsl' in version_info
        except:
            pass

    # ç¡®å®šè¿è¡Œæ—¶ç±»å‹
    if is_colab:
        runtime_type = 'colab'
    elif is_wsl:
        runtime_type = 'wsl'
    elif os_name == "Windows":
        runtime_type = 'windows'
    elif os_name == "Linux":
        runtime_type = 'linux'
    elif os_name == "Darwin":
        runtime_type = 'macos'
    else:
        runtime_type = 'unknown'

    logger.info(f"æ£€æµ‹åˆ°è¿è¡Œç¯å¢ƒ: {runtime_type}")

    return EnvironmentInfo(
        runtime_type=runtime_type,
        is_colab=is_colab,
        is_wsl=is_wsl,
        os_name=os_name,
        os_version=os_version
    )


def detect_gpu() -> GPUInfo:
    """æ£€æµ‹GPUä¿¡æ¯"""
    if not torch.cuda.is_available():
        logger.info("CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
        return GPUInfo(
            available=False,
            device_name="CPU",
            vram_total_gb=0.0,
            vram_available_gb=0.0,
            device_count=0
        )

    try:
        device_count = torch.cuda.device_count()
        device_name = torch.cuda.get_device_name(0)
        props = torch.cuda.get_device_properties(0)
        vram_total_gb = props.total_memory / 1024**3

        # è®¡ç®—èƒ½åŠ›ï¼ˆç”¨äºåˆ¤æ–­GPUæ€§èƒ½ï¼‰
        compute_capability = (props.major, props.minor)

        # è®¡ç®—å¯ç”¨æ˜¾å­˜
        torch.cuda.empty_cache()
        vram_allocated = torch.cuda.memory_allocated(0) / 1024**3
        vram_available_gb = vram_total_gb - vram_allocated

        logger.info(f"æ£€æµ‹åˆ°GPU: {device_name} ({vram_total_gb:.1f}GB VRAM)")

        return GPUInfo(
            available=True,
            device_name=device_name,
            vram_total_gb=vram_total_gb,
            vram_available_gb=vram_available_gb,
            device_count=device_count,
            compute_capability=compute_capability
        )

    except Exception as e:
        logger.warning(f"GPUæ£€æµ‹å¤±è´¥: {e}")
        return GPUInfo(
            available=False,
            device_name="CPU",
            vram_total_gb=0.0,
            vram_available_gb=0.0,
            device_count=0
        )


def detect_cpu() -> CPUInfo:
    """æ£€æµ‹CPUä¿¡æ¯ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    try:
        physical_cores = psutil.cpu_count(logical=False) or 1
        logical_cores = psutil.cpu_count(logical=True) or 1

        # è·å–CPUå‹å·
        cpu_model = platform.processor()
        if not cpu_model or cpu_model.strip() == "":
            # å¤‡ç”¨æ–¹æ³•
            import subprocess
            try:
                if platform.system() == "Windows":
                    result = subprocess.run(
                        ["wmic", "cpu", "get", "name"],
                        capture_output=True,
                        text=True
                    )
                    lines = result.stdout.strip().split('\n')
                    cpu_model = lines[1].strip() if len(lines) > 1 else "Unknown CPU"
                elif platform.system() == "Linux":
                    with open('/proc/cpuinfo', 'r') as f:
                        for line in f:
                            if 'model name' in line:
                                cpu_model = line.split(':')[1].strip()
                                break
                else:
                    cpu_model = "Unknown CPU"
            except:
                cpu_model = "Unknown CPU"

        # å°è¯•è·å–CPUç¼“å­˜å¤§å°ï¼ˆLinuxï¼‰
        cache_size_mb = None
        try:
            if platform.system() == "Linux":
                cache_info = Path("/sys/devices/system/cpu/cpu0/cache")
                if cache_info.exists():
                    # è¯»å–L3ç¼“å­˜ï¼ˆæœ€å¤§ç¼“å­˜å±‚çº§ï¼‰
                    for cache_dir in sorted(cache_info.glob("index*"), reverse=True):
                        size_file = cache_dir / "size"
                        if size_file.exists():
                            size_str = size_file.read_text().strip()
                            # è§£æå¦‚ "8192K" â†’ 8.0MB
                            if 'K' in size_str:
                                cache_size_mb = int(size_str.replace('K', '')) / 1024
                                break
        except:
            pass

        logger.info(f"æ£€æµ‹åˆ°CPU: {physical_cores}æ ¸ ({logical_cores}çº¿ç¨‹)")

        return CPUInfo(
            physical_cores=physical_cores,
            logical_cores=logical_cores,
            cpu_model=cpu_model,
            cache_size_mb=cache_size_mb
        )

    except Exception as e:
        logger.warning(f"CPUæ£€æµ‹å¤±è´¥: {e}")
        return CPUInfo(
            physical_cores=1,
            logical_cores=1,
            cpu_model="Unknown CPU"
        )


def detect_ram() -> RAMInfo:
    """æ£€æµ‹å†…å­˜ä¿¡æ¯"""
    try:
        mem = psutil.virtual_memory()
        total_gb = mem.total / 1024**3
        available_gb = mem.available / 1024**3
        percent_used = mem.percent

        logger.info(f"æ£€æµ‹åˆ°å†…å­˜: {total_gb:.1f}GB æ€»é‡, {available_gb:.1f}GB å¯ç”¨ ({100-percent_used:.1f}% ç©ºé—²)")

        return RAMInfo(
            total_gb=total_gb,
            available_gb=available_gb,
            percent_used=percent_used
        )

    except Exception as e:
        logger.warning(f"å†…å­˜æ£€æµ‹å¤±è´¥: {e}")
        return RAMInfo(
            total_gb=0.0,
            available_gb=0.0,
            percent_used=100.0
        )


def detect_hardware() -> HardwareInfo:
    """
    æ£€æµ‹å®Œæ•´ç¡¬ä»¶ä¿¡æ¯ï¼ˆå¢å¼ºç‰ˆï¼‰

    Returns:
        HardwareInfo: åŒ…å«GPUã€CPUã€å†…å­˜ã€ç¯å¢ƒç­‰å®Œæ•´ä¿¡æ¯

    Example:
        >>> hw = detect_hardware()
        >>> print(f"ç¯å¢ƒ: {hw.environment.runtime_type}")
        >>> print(f"GPU: {hw.gpu.device_name}, VRAM: {hw.gpu.vram_total_gb:.1f}GB")
        >>> print(f"CPU: {hw.cpu.physical_cores}æ ¸")
        >>> print(f"RAM: {hw.ram.total_gb:.1f}GB")
        >>> print(f"æ€§èƒ½æ¡£ä½: {hw.performance_tier}")
        >>> print(f"æ¨èè®¾å¤‡: {hw.recommended_device}")
    """
    logger.info("="*70)
    logger.info("æ­£åœ¨æ£€æµ‹ç¡¬ä»¶é…ç½®...")
    logger.info("="*70)

    environment = detect_environment()
    gpu = detect_gpu()
    cpu = detect_cpu()
    ram = detect_ram()

    hw_info = HardwareInfo(
        gpu=gpu,
        cpu=cpu,
        ram=ram,
        environment=environment,
        platform=platform.system(),
        python_version=platform.python_version()
    )

    logger.info("="*70)
    logger.info(f"ç¡¬ä»¶æ£€æµ‹å®Œæˆ - æ€§èƒ½æ¡£ä½: {hw_info.performance_tier}")
    logger.info("="*70)

    return hw_info


def print_hardware_summary(hw: HardwareInfo):
    """
    æ‰“å°ç¡¬ä»¶ä¿¡æ¯æ‘˜è¦ï¼ˆå¢å¼ºç‰ˆï¼‰

    Args:
        hw: HardwareInfoå¯¹è±¡
    """
    print("\n" + "="*70)
    print("ğŸ” ç¡¬ä»¶é…ç½®æ£€æµ‹ç»“æœ")
    print("="*70)

    # ç¯å¢ƒä¿¡æ¯ï¼ˆæ–°å¢ï¼‰
    print(f"ğŸŒ è¿è¡Œç¯å¢ƒ: {hw.environment.runtime_type.upper()}")
    if hw.environment.is_colab:
        print(f"  - Google Colab ç¯å¢ƒ")
    elif hw.environment.is_wsl:
        print(f"  - WSL (Windows Subsystem for Linux)")
    elif hw.environment.is_hospital_environment:
        print(f"  - åŒ»é™¢ç¯å¢ƒ (Windows)")
    print(f"  - ç³»ç»Ÿ: {hw.environment.os_name} {hw.environment.os_version}")

    # GPUä¿¡æ¯
    if hw.gpu.available:
        print(f"âœ“ GPU: {hw.gpu.device_name}")
        print(f"  - VRAM: {hw.gpu.vram_total_gb:.1f}GB (å¯ç”¨: {hw.gpu.vram_available_gb:.1f}GB)")
        print(f"  - CUDA: {hw.gpu.cuda_version}")
        if hw.gpu.compute_capability:
            print(f"  - è®¡ç®—èƒ½åŠ›: {hw.gpu.compute_capability[0]}.{hw.gpu.compute_capability[1]}")
        if hw.gpu.device_count > 1:
            print(f"  - è®¾å¤‡æ•°: {hw.gpu.device_count}ä¸ªGPU")
    else:
        print(f"âœ— GPU: ä¸å¯ç”¨")
        print(f"  - æ¨¡å¼: CPUæ¨ç†")

    # CPUä¿¡æ¯ï¼ˆå¢å¼ºï¼‰
    print(f"âœ“ CPU: {hw.cpu.physical_cores}æ ¸å¿ƒ ({hw.cpu.logical_cores}çº¿ç¨‹)")
    if hw.cpu.cpu_freq_max_mhz:
        print(f"  - é¢‘ç‡: {hw.cpu.cpu_freq_mhz:.0f}MHz (æœ€é«˜: {hw.cpu.cpu_freq_max_mhz:.0f}MHz)")
    elif hw.cpu.cpu_freq_mhz:
        print(f"  - é¢‘ç‡: {hw.cpu.cpu_freq_mhz:.0f}MHz")
    if hw.cpu.cache_size_mb:
        print(f"  - ç¼“å­˜: {hw.cpu.cache_size_mb:.1f}MB")
    if hw.cpu.architecture:
        print(f"  - æ¶æ„: {hw.cpu.architecture}")
    if hw.cpu.cpu_model and hw.cpu.cpu_model != "Unknown CPU":
        model_short = hw.cpu.cpu_model[:50] + "..." if len(hw.cpu.cpu_model) > 50 else hw.cpu.cpu_model
        print(f"  - å‹å·: {model_short}")

    # å†…å­˜ä¿¡æ¯
    print(f"âœ“ RAM: {hw.ram.total_gb:.1f}GB æ€»é‡")
    print(f"  - å¯ç”¨: {hw.ram.available_gb:.1f}GB ({100-hw.ram.percent_used:.1f}% ç©ºé—²)")
    if not hw.ram.is_sufficient:
        print(f"  âš ï¸  è­¦å‘Š: å¯ç”¨å†…å­˜ä¸è¶³6GBï¼Œå¯èƒ½å½±å“æ€§èƒ½")

    # å¹³å°ä¿¡æ¯
    print(f"âœ“ å¹³å°: {hw.platform}")
    print(f"âœ“ Python: {hw.python_version}")

    # æ€§èƒ½æ¡£ä½å’Œæ¨èé…ç½®ï¼ˆæ–°å¢ï¼‰
    print("\n" + "-"*70)
    print("ğŸ“Š æ€§èƒ½åˆ†æä¸æ¨è")
    print("-"*70)
    print(f"æ€§èƒ½æ¡£ä½: {hw.performance_tier}")
    print(f"æ¨èè®¾å¤‡: {hw.recommended_device.upper()}")
    print(f"æ¨è DataLoader workers: {hw.cpu.recommended_workers}")
    print(f"æ¨è Batch Size: {hw.ram.recommended_batch_size}")

    # åŒ»é™¢ç¯å¢ƒç‰¹åˆ«æç¤º
    if hw.environment.is_hospital_environment and not hw.gpu.available:
        print("\nâš•ï¸  åŒ»é™¢ç¯å¢ƒ CPU æ¨¡å¼ä¼˜åŒ–å»ºè®®:")
        if hw.cpu.physical_cores >= 8:
            print("  âœ“ CPUæ€§èƒ½è‰¯å¥½ (8æ ¸+)ï¼Œé¢„è®¡å¤„ç†é€Ÿåº¦: <60ç§’/æ‚£è€…")
        else:
            print("  âš ï¸  CPUæ ¸å¿ƒæ•°è¾ƒå°‘ï¼Œå»ºè®®ä½¿ç”¨8æ ¸+é…ç½®ä»¥è·å¾—æœ€ä½³æ€§èƒ½")

    print("="*70 + "\n")


def get_optimal_config(hw: HardwareInfo) -> Dict[str, Any]:
    """
    æ ¹æ®ç¡¬ä»¶ä¿¡æ¯è¿”å›æœ€ä¼˜é…ç½®ï¼ˆæ–°å¢å‡½æ•°ï¼ŒåŒ»é™¢ç¯å¢ƒå…³é”®ï¼‰

    Args:
        hw: ç¡¬ä»¶ä¿¡æ¯

    Returns:
        åŒ…å«æœ€ä¼˜é…ç½®çš„å­—å…¸
    """
    config = {
        'device': hw.recommended_device,
        'num_workers': hw.cpu.recommended_workers,
        'batch_size': hw.ram.recommended_batch_size,
        'pin_memory': hw.gpu.available,  # GPUæ—¶å¯ç”¨pin_memory
        'prefetch_factor': 2 if hw.cpu.recommended_workers > 0 else None,
        'performance_tier': hw.performance_tier,
    }

    # CPUä¼˜åŒ–ï¼ˆåŒ»é™¢ç¯å¢ƒï¼‰
    if not hw.gpu.available:
        config['cpu_optimization'] = {
            'torch_threads': hw.cpu.physical_cores,  # PyTorchçº¿ç¨‹æ•°
            'mkl_threads': hw.cpu.physical_cores,    # MKLçº¿ç¨‹æ•°ï¼ˆIntel CPUåŠ é€Ÿï¼‰
            'omp_threads': hw.cpu.physical_cores,    # OpenMPçº¿ç¨‹æ•°
        }

    return config


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logging.basicConfig(level=logging.INFO, format='%(message)s')

    print("\næµ‹è¯•ç¡¬ä»¶æ£€æµ‹æ¨¡å—ï¼ˆå…±äº«ç‰ˆï¼‰...")
    print("="*70)

    hw = detect_hardware()
    print_hardware_summary(hw)

    # è·å–æœ€ä¼˜é…ç½®
    print("\næœ€ä¼˜é…ç½®:")
    print("="*70)
    optimal_config = get_optimal_config(hw)
    for key, value in optimal_config.items():
        print(f"{key}: {value}")

    # éªŒæ”¶æ ‡å‡†
    print("\néªŒæ”¶æ ‡å‡†æ£€æŸ¥:")
    print(f"âœ“ ç¯å¢ƒæ£€æµ‹: {hw.environment.runtime_type}")
    print(f"âœ“ GPUå¯ç”¨: {hw.gpu.available}")
    print(f"âœ“ VRAM: {hw.gpu.vram_total_gb:.1f}GB")
    print(f"âœ“ CPUæ ¸å¿ƒ: {hw.cpu.physical_cores}")
    print(f"âœ“ å†…å­˜: {hw.ram.total_gb:.1f}GB")
    print(f"âœ“ æ€§èƒ½æ¡£ä½: {hw.performance_tier}")
    print(f"âœ“ æ¨èworkers: {hw.cpu.recommended_workers}")
